import re
import json
import datetime
import time
import math
from operator import itemgetter
from copy import copy, deepcopy
from collections import defaultdict
from hashlib import md5
from celery.decorators import task

import HTMLParser
from dateutil.parser import parse as dateutil_parse

from django.conf import settings
from django.core.cache import cache, get_cache
from django.core.urlresolvers import reverse
from django.db.models import Q

from debra import constants, account_helpers, mongo_utils
from debra.constants import STRIPE_PLAN_CHEAP, STRIPE_PLAN_BASIC
from debra.constants import (
    ELASTICSEARCH_OFFSET_CACHE_TIMEOUT, ELASTICSEARCH_POP_CACHE_TIMEOUT)
from debra.constants import SEARCH_FLYOUT_POST_LENGTH_LIMIT
from debra import elastic_search_helpers
from debra.elastic_search_helpers import (
    es_influencer_query_runner_v2, es_post_query_runner_v2,
    es_product_query_runner_v2)
from debra import logical_categories
from xpathscraper.utils import domain_from_url
from debra.models import User

html_parser = HTMLParser.HTMLParser()
mc_cache = get_cache('memcached')
redis_cache = get_cache('redis')


''' Elasticsearch ES filtered_keyword_search
 
Searches are exposed as single methods, that take standardized arguments, and
are named accordingly. For example, search_influencers() accepts search_query,
which is produced by the frontend javascript application, and items_per_page,
which limits the number of results returned per page. search_influencer_posts,
on the other hand, requires a debra.models.Influencer object to be passed in
addition to the above parameters.

search_influencer_products is similar in naming and behavior.

There are legacy methods that are still called from elsewhere; The search_ methods
are the preferred route forward. See es_frontend_emulator.py for an API for building
queries that these methods accept (the emulator also has helpers that access these
methods by the same name-- see documentation).

There are a few underlying methods that all of the search_ methods rely on, as well
as a common interface that the methods themselves must implement. They really should be
objects-- A functional pattern was used to reuse as much existing code as possible.

Any method that ends in _search is used by all searching methods early in the process.
They build a standardized set of options to pass into the elastic_search_methods.py 
methods that are detailed in the documentation of that file. While there are multiple
methods with the _search nomenclature, none of them should be called individually,
as they are called in sequence by esquery_options, and only two should see changes
when routinely adding options to the front end search mehanisms: filtered_search and
keyword_search. The former transforms frontend filters (things passed inside the 
'filters' object) into filtering options; The latter does likewise for 'keyword'.

Note the nomenclature of the options: A filter is prefixed with filter_ and a keyword
is prefixed with keyword_. This is important, as it is used for the method dispatch in
elastic_search_helpers.py.

The common interface implemented by the search methods is as follows:
    There must be a unique name for the search type. It is referenced as index_type in
    this file, but should not be confused with index_type in elastic_search_helpers.

    There must be a search_query, which is the query generated by the frontend.

    There must be an items_per_page limit, which is currently 60 for influencers,
    12 for influencer posts, and 25 for influencer products.

    There must be a method that can be called that returns items from elasticsearch for
    any given search_query. The name of this method is not technically important, but
    should be named search_thing_fetcher. This method may be called repeatedly while
    building a full result set, so if any persistent data is needed for it, pass it in
    a closure. See search_influencers_fetcher for an example, where searchable_influencers,
    which are retrieved from cache or postgres, are passed into the closure and a new
    method is returned from the closure.

    There must be a method that is used to model elasticsearch data into postgres data,
    as all items are queried from postgres. The same closure rule applies here. These
    methods end in _modeler.

    There must be a method that is called on the collection of modeled data, which follows
    the closure rules above. This method can pass, but usually will need to do something,
    and for consistency are named like search_thing_processor.

Before examining what the current implementations of the methods do that are passed
into build_result (the methods above), a breakdown of how build_result uses these parameters:
   The index type is used for cache keys. Currently, only search_influencers_modeler caches.

   The search_query is used by the fetcher method to build an elasticsearch query.

   build_result attempts to build a slice of results which fulfils the items_per_page
   paramters by continuously offsetting the start and length of the next ES query. For example,
   if asking for 10 items:

       The first query will ask for 1-10.
       The next query will ask for 11-30.
       The next query will ask for 31-60.

   Every time a set of results is returned from ES, it is passed into the modeler method, which
   should return a serialized form of the data from postgres. Then, that modeled data has
   processor called on it, which handles sorting/highlighting.

   At the end of the method sequence, if the set of results does not fulfil the items_per_page
   requirement, then another iteration is run. This is where the offsetting above comes into
   play, and is why it is needed to create the closures for the fetcher/modeler/processor methods.

   The offset is cached (under index_type), and is used to offset pagination of requests that come
   in the future. For example, if the modeler for page one pruned 12 results from the set, then
   page two will automagically start at item 72 for a query limited to 60 results.

The search_influencers_fetcher, _modeler, and _processor are all pretty straight forward, but the
search_influencer_posts/products_fetcher, _modeler, and _processor methods should be carefully
studied prior to alteration. These constructor functions selectively call other constructor
functions, which are the ones that return the closure.

In general: _fetchers get from ES, _modelers get from Postgres (usually using debra.models), and
_processor prepares the data for the result set that will be returned.


The current fetchers call methods named filtered_influencer_search,
filtered_influencer_posts_search, and filtered_influencer_products_search, which
are the entry points into the elastic_search_helpers.py file via query_influencers,
query_posts, and query_products. The methods in the helper file are designed to be
more generic than those of this file; For example, a future implementation of 
search_post_products would build options to be passed into query_products.

The calls to query_influencers/posts/products are dynamically dispatched using the 
filtered_keyword_search method. This ensures consistent processing of the options, etc.
All of these methods return the same three things:
    - debra.models.Model filtered by a Q() (see query_q in elastic_search_helpers).
    - a list of ordered attributes, according to ES, such as url, id, or name.
    - the total number of hits for the search, used in pagination and displayed to the front end.
'''
def single_to_plural(item):
    if type(item) == list:
        return item
    return [item]


@task(name="debra.search_helpers.user_query_stats_helper", ignore_result=True)
def user_query_stats_helper(request, search_query, base_brand):
    """
    This method tracks the stats for a brand's behavior.
    We moved this out from the search_views.blogger_search_json and to a celery task so that the view can finish faster.
    """

    # print("Got: request %r" % request)
    print("Got search_query %r" % search_query)
    print("Got base_brand %r" % base_brand)

    mongo_utils.track_visit(request)

    # first prettify the query for mandrill, intercom, and slack
    try:
        only_setup_params = find_non_default_query(search_query)
        if only_setup_params is None or only_setup_params == [{}]:
            only_setup_params = {}
        query_formatted = format_query_for_displaying(only_setup_params)
        print "only_setup_params = [%r]  query_formatted = [%r]" % (only_setup_params, query_formatted)
    except:
        a = json.dumps(search_query, sort_keys=True, indent=4, separators=(',', ': '))
        query_formatted = 'Problem in formatting %r' % a
        pass

    mongo_utils.track_query("brand-search-query", query_formatted, {"user_id": request.visitor["auth_user"].id})

    account_helpers.intercom_track_event(request, "brand-search-query", {
        'query': query_formatted,
    })

    if base_brand:
        user = User.objects.get(id=request.user.id)
        if base_brand.flag_trial_on and not account_helpers.internal_user(user):
            slack_msg = "\n**************\nBrand = " + base_brand.domain_name + " User: " + request.user.email + "\n" + query_formatted
            account_helpers.send_msg_to_slack('brands-trial-activity', slack_msg)

        base_brand.saved_queries.create(query=json.dumps(search_query), user=request.user)



def find_non_default_query(query):
    """
    Here, we remove default settings from the query and return it.
    Default query looks like this:
        {u'filters': {u'brand': [],
                      u'engagement': [],
                      u'gender': [],
                      u'popularity': [],
                      u'priceranges': []},
         u'keyword': None,
         u'order_by': {},
         u'page': 1,
         u'type': None}
    """
    nq = deepcopy(query)
    keys = nq.keys()

    def del_kk(k, m):
        if not m[k]:
            print("Deleting %s[%s]" % (m, k))
            del m[k]
            return
        if k == 'page' and m[k] and m[k] == 1:
            del m[k]
            return
        if k == 'type' and m[k] and m[k] == 'all':
            del m[k]
            return
        if k == 'order':
            del m[k]
            return
        if k == 'group_concatenator':
            del m[k]
            return
        if k == 'and_or_filter_on':
            del m[k]
            return
        if k == 'sub_tab':
            del m[k]
            return
        if k == 'search_method' and m[k] == 'default':
            del m[k]
            return

    for k in keys:
        print("Trying key k %s" % k)
        v = nq[k]
        if k == 'filters':
            print("Now, looking at %s" %v)
            kn = v.keys()
            print("Now, keys are %s" % kn)
            for kk in kn:
                del_kk(kk, v)

        del_kk(k, nq)
    print("Final query left: %s" % nq)
    return nq


def format_query_for_displaying(query):
    """
    Our home-grown method to print out the JSON query in a short-form on slack so that we can quickly figure out
    what the query is.
    """
    keys = query.keys()
    result = ''
    for k in keys:
        v = query[k]
        print("Checking v=%r and k=%r" % (v, k))
        result += '[%s] ' % k
        if v and type(v) == dict:
            kk = v.keys()
            r = ''
            for l in kk:
                m = v[l]
                r += str(m)
                r += ' '
            result += r
        else:
            print("result: [%s] v:[%s]" % (result, str(v)))
            result += str(v)
        result += '\n'
    if result == '':
        result = 'JUST VISITED THE SEARCH PAGE.'
    return result

def brand_from_keyword(keyword):
    ''' brand_from_keyword returns brand models if they match the keywords. '''
    from debra.models import Brands
    if type(keyword) == dict:
        keyword = keyword.get("value")

    domain = domain_from_url(keyword)

    try:
        brand = Brands.objects.get(
            blacklisted=False, domain_name=domain)
    except Exception as e:
        brand = None

    return brand
        
def hashtag_from_keyword(keyword, force=False):
    ''' hashtag_from_keyword return a hashtag from keyword, or forces a hashtag from keyword. '''    
    if keyword.startswith("#"):
        return keyword
    elif force:
        return "#" + keyword
    else:
        return None
        
def mention_from_keyword(keyword, force=False):
    ''' mention_from_keyword returns a mention from keyword, or forces a mention from keyword. '''        
    if keyword.startswith("@"):
        return keyword
    elif force:
        return "@" + keyword
    else:
        return None

def sort_results_by(data, key, mapping, reverse=False):
    data.sort(key=lambda thing: mapping[thing[key]], reverse=reverse)
    
def get_order_by(options):
    def get_order(o):
        if o.lower() == 'asc':
            return 'asc'
        else:
            return 'desc'

    simple = ['_score']
    nested = [(('social_platforms.num_followers', 'sum'),
               'social_platforms.name', [('facebook_followers', 'Facebook'),
                                          ('twitter_followers', 'Twitter'),
                                          ('pinterest_followers', 'Pinterest'),
                                          ('instagram_followers', 'Instagram')])]
              
    option = options.get('order_by')
    order = {}

        
    if option:
        if option['field'] in simple:
            order[option['field']] = {'order': get_order(option['order'])}
        else:
            for ((field, mode), term, opts) in nested:
                for (alias, name) in opts:
                    if option['field'] == alias:
                        order[field] = {
                            'mode': mode,
                            'order': get_order(option['order']),
                            'nested_filter': {'term': {term: name}}
                        }
                        break
                if order:
                    break

        if not order:
            del options['order_by']

    return order if order else None
        

def get_filter(filters, name):
    return filters[name] if name in filters and filters[name] else None
        
def prepare_filter_params(context, plan_name=None, **kw):
    """
    returns json used to populate filter panel

    NOTE => we're using one key 'filter_param' to store the filters that will be used by ALL clients.
    """
    from debra.models import Influencer
    from debra import logical_categories
    from django.core.cache import get_cache
    cache = get_cache('memcached')
    params = None #cache.get('filter_params')
    if not params:
        # influencers = Influencer.objects.filter(
        #     show_on_search=True).exclude(blacklisted=True)
        # influencers = influencers.filter(
        #     score_popularity_overall__isnull=False)
        # influencers = influencers.distinct()
        popularity = [
            {
                "title": "Small",
            },
            {
                "title": "Medium",
            },
            {
                "title": "Large",
            }
        ]
        engagement = [
            {
                "title": "0-20",
            },
            {
                "title": "21-40",
            },
            {
                "title": "41-60",
            },
            {
                "title": "61-80",
            },
            {
                "title": "81+",
            },
        ]

        price_ranges = [
            {
                 "title": "Cheap",
                 "text": "Primarily In-expensive"
            },
            # {
            #      "title": "Mid-level",
            # },
            {   
                "title": "Expensive",
                "text": "Primarily High-end"
            }
        ]

        genders = [
            {
                "title": "Female",
            },
            {
                "title": "Male",
            },
        ]

        social = [
            {
                "value": "Facebook",
                "icon": "icon-social_facebook"
            },
            {
                "value": "Pinterest",
                "icon": "icon-social_pinterest2"
            },
            {
                "value": "Twitter",
                "icon": "icon-social_twitter"
            },
            {
                "value": "Instagram",
                "icon": "icon-social_instagram2"
            },
            {
                "value": "Youtube",
                "icon": "icon-social_youtube"
            },
        ]

        age_groups = [
            {
                "value": "0_19",
                "icon": "0 - 19"
            },
            {
                "value": "20_24",
                "icon": "20 - 24"
            },
            {
                "value": "25_29",
                "icon": "25 - 29"
            },
            {
                "value": "30_34",
                "icon": "30 - 34"
            },
            {
                "value": "35_39",
                "icon": "35 - 39"
            },
            {
                "value": "40",
                "icon": "40+",
            }
        ]

        activity = [{"value": "Blog", "icon": "icon icon-letter_quotes2"}] + social

        categories = []

        brands = []

        locations = redis_cache.get('toplocs') or []
        # locations = Influencer.get_locations_list(num_results=200)
        # locations = Influencer.get_locations_list(num_results=None)

        tags = kw.get('tags', [])

        source = [{"title": "Signup", "value": "blogger_signup"}]

        params = {
            'show_filters': True,
            'popularity': list(popularity),
            'engagement': list(engagement),
            'categories': list(categories),
            'brands': list(brands),
            'priceranges': list(price_ranges),
            'locations': list(locations),
            'genders': list(genders),
            'social': list(social),
            'activity': list(activity),
            'tags': list(tags),
            'source': list(source),
            'age_groups': list(age_groups),
            'enabled_filters': [
                "popularity", "engagement", "categories", "brands",
                "priceranges", "location", "genders", "socials", "activity",
                "tags", "likes", "shares", "comments", "source", "avgAge",
                "customCategories", "customOccupation", "customSex", "customEthnicity",
                "customTags", "customLanguage", "customAgeRange",]
        }
        cache.set('filter_params', params)

    for loc in params.get('locations', []):
        loc['value'] = loc['title']

    if True: #settings.DEBUG:
        params['categories'] =  [{"title": "Fashion", "category": "fashion"},
                                 {"title": "Food", "category": "food"},
                                 {"title": "Kids", "category": "kids"},
                                 {"title": "Beauty", "category": "beauty"},
                                 {"title": "Travel", "category": "travel"}]
    else:
        params['categories'] = []
        
    return params

RE_IMG_ELEM = re.compile(r"(<(\s*?)img([^>]*?)>)", flags=re.M | re.I | re.S)
RE_NL = re.compile(r"\n", flags=re.M | re.I | re.S)
RE_SCRIPT = re.compile(
    r"(<(\s*?)script([^>]*?)>.*?<\s*?\/\s*?script([^>]*?)>)", flags=re.M | re.I | re.S)
RE_STYLE = re.compile(
    r"(<(\s*?)style([^>]*?)>.*?<\s*?\/\s*?style([^>]*?)>)", flags=re.M | re.I | re.S)
RE_XML = re.compile(
    r"(<(\s*?)xml([^>]*?)>.*?<\s*?\/\s*?xml([^>]*?)>)", flags=re.M | re.I | re.S)
RE_IFRAME = re.compile(
    r"(<(\s*?)iframe([^>]*?)>.*?<\s*?\/\s*?iframe([^>]*?)>)", flags=re.M | re.I | re.S)
RE_TAG = re.compile(r"(<([^>]+)>)", flags=re.M | re.I | re.S)
RE_IMG_URL = re.compile(
    r"\b((https?|ftp)://[^\s/$.?#].[^\s]*.(jpe?g|png|gif))\b", flags=re.M | re.I | re.S)
RE_LINK = re.compile(
    r"\b((https?|ftp)://[^\s/$.?#].[^\s]*.)\b", flags=re.M | re.I | re.S)
RE_IMG_SRC = re.compile(r"<img.*?src=['\"](.*?)['\"].*?>")


def tagStripper(txt, length_limit=20):
    """
    takes text (html) and optionally max output text length
    return tuple of html tags stripped text and list of images found in text
    """
    if not txt:
        return "", []
    txt = html_parser.unescape(txt)
    imgs = [x[0] for x in RE_IMG_ELEM.findall(txt)]
    txt = RE_NL.sub(" ", txt)
    txt = RE_SCRIPT.sub(" ", txt)
    txt = RE_STYLE.sub(" ", txt)
    txt = RE_XML.sub(" ", txt)
    txt = RE_IFRAME.sub(" ", txt)
    txt = RE_TAG.sub(" ", txt)
    imgs_url = [x[0] for x in RE_IMG_URL.findall(txt)]
    txt = RE_LINK.sub(" ", txt)
    oryg_len = len(txt.split())
    txt = " ".join(txt.split()[:length_limit])
    if len(txt.split()) < oryg_len:
        txt += "..."
    for img in imgs:
        imgs_url.extend(RE_IMG_SRC.findall(img))
    return txt, imgs_url

import logging
log = logging.getLogger('debra.search_helpers')


def get_post_platform(post):
    domain = domain_from_url(post.url)
    mapping = {
        'facebook.com': 'Facebook',
        'instagram.com': 'Instagram',
        'twitter.com': 'Twitter',
        'pinterest.com': 'Pinterest',
        'youtube.com': 'Youtube',
    }
    try:
        return mapping[domain]
    except KeyError:
        return 'Blog'


def serializePostsData(influencer, posts, length_limit=30, highlight=False):
    """
    takes influencer instance and posts querystring, optionally maximum content length
    returns json serializable dictionary
    """
    from debra import serializers

    posts_data = []
    urls = set()
    posts = list(posts)
    dated = []
    undated = []
    for post in posts:
        if post.create_date:
            dated.append(post)
        else:
            undated.append(post)

    posts = sorted(dated, key=lambda x: x.create_date)
    posts.reverse()
    posts.extend(undated)

    if length_limit:
        length_limit = length_limit

    for post in posts:
        if post.url in urls:
            continue
        urls.add(post.url)
        post_data = {}
        post_data["post_image"] = post.post_image
        stripped_content, images = tagStripper(
            post.content, length_limit=length_limit)
        post_data["content"] = stripped_content
        post_data["content_images"] = images
        post_data["url"] = post.url
        post_data["blog_name"] = serializers.unescape(influencer.blogname if influencer else\
            post.influencer.blogname)
        post_data["title"] = post.title
        post_data["platform"] = get_post_platform(post)
        if highlight:
            post_data["highlight"] = True
        if post.create_date:
            post_data["create_date"] = post.create_date.strftime("%b. %e, %Y")
        if not influencer:
            post_data['user'] = post.influencer.feed_stamp
        if post.products_json:
            post_data["products"] = post.get_product_json()
        else:
            post_data["products"] = []
        posts_data.append(post_data)
    return posts_data


def serialize_posts_data_v2(influencer, posts, length_limit=30, highlighted_ids=[], **kw):
    """
    takes influencer instance and posts querystring, optionally maximum content length
    returns json serializable dictionary
    """
    from debra import serializers
    from debra import feeds_helpers
    from debra import constants

    request = kw.get('request')
    brand = request.visitor["base_brand"] if request else None

    posts_data = []
    urls = set()
    posts = list(posts)

    for post in posts:
        if post.url in urls:
            continue
        urls.add(post.url)

        feed_json = feeds_helpers.get_feed_handler_for_platform(
            get_post_platform(post))

        post_data = feed_json(None,
            for_single_post=post,
            length_limit=length_limit
        )

        if post_data is None:
            continue

        post_data["blog_name"] = serializers.unescape(influencer.blogname if influencer else post.influencer.blogname)
        post_data["title"] = post.title
        post_data["platform"] = get_post_platform(post)

        if brand and brand.flag_show_dummy_data:
            post_data['url'] = constants.FAKE_POST_DATA['url']
            post_data['title'] = constants.FAKE_POST_DATA['title']

        if post.id in highlighted_ids:
            post_data["highlight"] = True
        if post.create_date:
            post_data["create_date"] = post.create_date.strftime("%b. %e, %Y")
        if influencer:
            post_data['user'] = influencer.feed_stamp
        else:
            post_data['user'] = post.influencer.feed_stamp
        posts_data.append(post_data)
    return posts_data


def serializeItemsData(items, highlight=False):
    """
    takes items querystring
    returns json serializable dictionary
    """
    from debra.models import ProductModelShelfMap
    #items = items.filter(added_datetime__gte=datetime.date.today()-datetime.timedelta(days=30))
    # unordered_pair = list(items.values_list('added_datetime', 'id'))
    unordered_pair = []

    for item in items:
        unordered_pair.append((item.added_datetime, item.id))

    unordered_pair.sort()
    unordered_pair.reverse()
    ids = [x[1] for x in unordered_pair[:60]]
    items = ProductModelShelfMap.objects.select_related(
        'product_model__brand').filter(id__in=ids)
    items_data = []
    prod_model_existing = set()
    for item in items:
        if item.product_model.name in prod_model_existing:
            continue
        prod_model_existing.add(item.product_model.name)
        item_data = {
            "name": item.product_model.name,
            "img_url_feed_view": item.product_model.img_url,
            "img_url_panel_view": item.img_url_panel_view,
        }
        if highlight:
            item_data["highlight"] = True
        if item.product_model.brand:
            item_data["brand"] = item.product_model.brand.name
        items_data.append(item_data)
    return items_data


def serialize_items_data_v2(items, highlighted_ids=[]):
    """
    takes items querystring
    returns json serializable dictionary
    """
    items_data = []
    prod_model_existing = set()
    for item in items:
        if item.product_model.name in prod_model_existing:
            continue
        prod_model_existing.add(item.product_model.name)
        item_data = {
            "name": item.product_model.name,
            "img_url_feed_view": item.product_model.img_url,
            "img_url_panel_view": item.img_url_panel_view,
        }
        if item.id in highlighted_ids:
            item_data["highlight"] = True
        if item.product_model.brand:
            item_data["brand"] = item.product_model.brand.name
        items_data.append(item_data)
    return items_data


def get_social_data(influencer=None, profile=None, **kw):
    """
    takes optional influencer and profile
    returns dictionary of all social data for provided input we have
    """
    social_data = {}
    twitter_page = None
    facebook_page = None
    instagram_page = None
    pinterest_page = None
    youtube_page = None

    request = kw.get('request')
    brand = request.visitor["base_brand"] if request else None

    if profile:
        social_data["etsy_page"] = profile.etsy_page
        social_data["store_page"] = profile.store_page
        social_data["facebook_page"] = profile.facebook_page
        social_data["twitter_page"] = profile.twitter_page
        social_data["pinterest_page"] = profile.pinterest_page
        social_data["web_page"] = profile.web_page
        social_data["etsy_page"] = profile.etsy_page
        social_data["store_page"] = profile.store_page
        social_data["instagram_page"] = profile.instagram_page
        social_data["bloglovin_page"] = profile.bloglovin_page
        social_data["youtube_page"] = profile.youtube_page

    # not sure if this is a BUG, but seems to be it
    # if we miss some url in profile, we should try to get it from influencer

    if influencer:
        if not social_data.get("facebook_page"):
            social_data["facebook_page"] = influencer.fb_url
        if not social_data.get("pinterest_page"):
            social_data["pinterest_page"] = influencer.pin_url
        if not social_data.get("twitter_page"):
            social_data["twitter_page"] = influencer.tw_url
        if not social_data.get("instagram_page"):
            social_data["instagram_page"] = influencer.insta_url
        if not social_data.get("bloglovin_page"):
            social_data["bloglovin_page"] = influencer.bloglovin_url
        if not social_data.get("youtube_page"):
            social_data["youtube_page"] = influencer.youtube_url

    # if influencer:
    #     if "facebook_page" not in social_data:
    #         social_data["facebook_page"] = influencer.fb_url
    #     if "pinterest_page" not in social_data:
    #         social_data["pinterest_page"] = influencer.pin_url
    #     if "twitter_page" not in social_data:
    #         social_data["twitter_page"] = influencer.tw_url
    #     if "instagram_page" not in social_data:
    #         social_data["instagram_page"] = influencer.insta_url
    #     if "bloglovin_page" not in social_data:
    #         social_data["bloglovin_page"] = influencer.bloglovin_url
    #     if "youtube_page" not in social_data:
    #         social_data["youtube_page"] = influencer.youtube_url

        platforms_qs = influencer.get_platform_for_search
        for platform in platforms_qs:
            if platform.platform_name == "Twitter":
                twitter_page = platform.url
            elif platform.platform_name == "Facebook":
                facebook_page = platform.url
            elif platform.platform_name == "Instagram":
                instagram_page = platform.url
            elif platform.platform_name == "Pinterest":
                pinterest_page = platform.url
            elif platform.platform_name == "Youtube":
                youtube_page = platform.url

        if facebook_page:
            social_data["facebook_page"] = facebook_page
        if twitter_page:
            social_data["twitter_page"] = twitter_page
        if instagram_page:
            social_data["instagram_page"] = instagram_page
        if pinterest_page:
            social_data["pinterest_page"] = pinterest_page
        if youtube_page:
            social_data["youtube_page"] = youtube_page
        social_data["bloglovin_page"] = influencer.bloglovin_url
        #social_data["youtube_page"] = influencer.youtube_url
        social_data["lb_page"] = influencer.lb_url

    # social_data["has_social"] = any(social_data.values())

    # if profile:
    #     social_data["blog_name"] = profile.blog_name
    # if influencer:
    #     social_data["blog_page"] = influencer.blog_url

    if brand and brand.flag_show_dummy_data:
        for k, v in social_data.items():
            if v:
                social_data[k] = constants.FAKE_BLOGGER_DATA['social_url']

    social_data["has_social"] = any(social_data.values())

    if profile:
        social_data["blog_name"] = profile.blog_name
    if influencer:
        social_data["blog_page"] = influencer.blog_url

    if brand and brand.flag_show_dummy_data:
        if social_data.get("blog_name"):
            social_data["blog_name"] = constants.FAKE_BLOGGER_DATA['blogname']
        if social_data.get("blog_page"):
            social_data["blog_page"] = constants.FAKE_BLOGGER_DATA['blog_url']

    print social_data
    return social_data


def search_results(data_mapping, ordered_ids):
    ''' search_results returns an ordered list from data_mapping based on ordered_ids. '''
    results = []

    for item_id in ordered_ids:
        if not item_id in data_mapping:
            try:
                item_id = int(float(item_id))
            except ValueError as e:
                continue

        if item_id in data_mapping:
            results.append(data_mapping[item_id])    
    return results

def social_platforms_ordering(platform):
    ''' social_platforms_ordering returns a nested sorting filter for followers. '''
    return {
        'field': 'social_platforms.num_followers',
        'mode': 'max',
        'nested_filter': {
            "term": {
                'social_platforms.name': platform.capitalize()
            }
        }
    }

    
def available_orders():
    ''' available_orders returns the supported ordering modes. '''
    return {
        'popularity': {
            'field': 'popularity'
        },
        'twitter_followers': social_platforms_ordering('Twitter'),
        'facebook_followers': social_platforms_ordering('Facebook'),
        'pinterest_followers': social_platforms_ordering('Pinterest'),
        'instagram_followers': social_platforms_ordering('Instagram'),
        'youtube_followers': social_platforms_ordering('Youtube'),
        '_score': {
            'field': '_score'
        },
        'create_date': {
            'field': 'create_date'
        },
        'insert_date': {
            'field': 'insert_date'
        }
    }

def get_direction(order):
    ''' get_direction returns a normalized direction. '''
    if 'order' in order:
        if order['order'].lower() == 'asc':
            return 'asc'

    return 'desc'
            
def ordered_search(options, esquery):
    ''' Prepares sort parameters for ES searches.'''    
    order_by = esquery.get('order_by', {})
    if order_by:
        available = available_orders()
            
        if order_by['field'] in available:
            field = available[order_by['field']]

            if 'mode' in field:
                options['order_by'] = {
                    field['field']: {
                        'mode': field['mode'],
                        'nested_filter': field['nested_filter'],
                        'order': get_direction(order_by)
                }}
            else:
                options['order_by'] = {
                    field['field']: {
                        'order': get_direction(order_by)
                    }
                }

def get_searchable_influencers(order_by='-score_popularity_overall'):
    ''' Returns a list of searchable influencers from postgres. '''                
    from debra.models import Influencer
    started = time.time()
    
    if settings.DEBUG:
        print 'ES getting searchable influencers'
    
    influencers = Influencer.objects.only('id')

    # Temporary fix to keep new activity levels from showing on production.
    if settings.DEBUG:
        influencers = influencers.filter(show_on_search=True)
    else:
        # we made sure that all influencer.show_on_search=True are
        # marked old_show_on_search=True at this point and these are
        # the only ones that will continue to show up on search while
        # we upgrade other influencers on show_on_search=True
        influencers = influencers.filter(old_show_on_search=True)

    influencers = influencers.filter(profile_pic_url__isnull=False,
                                     score_popularity_overall__isnull=False
                                 ).exclude(
                                     blacklisted=True).exclude(source__icontains='brands'
                                 ).order_by(order_by)

    if settings.DEBUG:
        print 'ES got searchable influencers in ', time.time() - started
        
    return influencers

def get_popularity_filter(popularity, influencers):
    ''' get_popularity_filter returns the popularity cutoffs for popularity searches. '''
    popularity = popularity.lower()
    
    if popularity in ('small', 'medium', 'large'):
        first = cache.get('one_third_popularity')
        second = cache.get('two_thirds_popularity')

        if first is None or second is None:
            count = influencers.count()
            one_third = count / 3
            two_third = count * 2 / 3

            sorted_by_pop = influencers.order_by(
                'score_popularity_overall'
            )

            first = sorted_by_pop[one_third].score_popularity_overall
            second = sorted_by_pop[two_third].score_popularity_overall

            cache.set('one_third_popularity', first, ELASTICSEARCH_POP_CACHE_TIMEOUT)
            cache.set('two_thirds_popularity', second, ELASTICSEARCH_POP_CACHE_TIMEOUT)

        return {
            'small': {
                'max': first
            },
            'medium': {
                'min': first,
                'max': second
            },
            'large': {
                'min': second
            }
        }.get(popularity, None)
            
    return None

def get_price_filter(price):
    ''' get_price_filter returns a normalized price filter for ES from a frontend query. '''
    price = price.lower()

    if price in ('cheap', 'mid-level', 'expensive'):
        return price if price != 'mid-level' else "middle"
    return None
        
def get_md5_from_list(components):
    ''' get_md5_from_list returns an md5 cahce key for a list of components (must be unique combo). '''    
    key = ""

    for component in sorted(components):
        key += str(component)
        
    return md5(key).hexdigest()    

def collect_esquery(esquery, page):
    ''' collect_esquery collects order_by, filters, brand, and keyword (no pagination) into a list. '''
    collected = []

    if page:
        collected.append(page)
    elif 'page' in esquery:
        collected.append(esquery['page'])
        
    if 'type' in esquery:
        if type(esquery['type']) == list:
            collected.extend(esquery['type'])
        else:
            collected.append(esquery['type'])
    
    if 'order_by' in esquery:
        for key, field in esquery['order_by'].iteritems():
            collected.append(key)
            collected.extend(field)

    if 'filters' in esquery:
        for name, f in esquery['filters'].iteritems():
            collected.append(name)

            if type(f) == list:
                collected.extend(f)
            else:
                collected.append(f)

    return collected

def get_pagination_offset(index_type, esquery, page=None):
    ''' get_pagination_offset returns the cached pagination offset for esquery or 0. '''
    if '_offset' in esquery:
        offset = esquery['_offset']
        del esquery['_offset']
        return offset
   
    return cache.get(get_md5_from_list(collect_esquery(esquery, page) + [index_type]), 0)

def set_pagination_offset(index_type, esquery, offset, page=None):
    ''' set_pagination_offset sets the offset for ES queries for a given frontend query. '''
    if offset:
        cache.set(get_md5_from_list(collect_esquery(esquery, page) + [index_type]),
                  offset,
                  ELASTICSEARCH_OFFSET_CACHE_TIMEOUT)
    
def paginated_search(index_type, options, esquery):
    ''' Prepares pagination options for ES searches. '''
    page = esquery.get('page', 1)
    count = esquery.get('count', 60)

    options['from'] = ((page * count) - count) + get_pagination_offset(index_type, esquery)
    options['size'] = count
    

def filtered_search(options, esquery, influencer=None, influencers=None, key=None, **kwargs):
    ''' Prepares filter options for ES searches.
    
    esquery is passed from the front end as JSON in the q param or post body
    options are the options to be passed to the query builder
    influencer (debra.models.Influencer) filters by blogger_id
    '''
    
    filters = esquery.get('filters', {})
    brands = filters.get('brand', [])
    num_comments = filters.get('engagement', {})
    price = filters.get('priceranges', [])
    location = filters.get('location', [])
    gender = filters.get('gender', [])
    social_followers = filters.get('social', [])
    social_activity = filters.get('activity', {})
    categories = filters.get('categories', [])
    popularity = filters.get('popularity', [])

    if brands:
        options['filter_brand'] = filter(None,
                                         map(extract_brand_name,
                                             [b['value'].strip() for b in brands]
                                         ))
    if num_comments:
        if type(num_comments) != list:
            num_comments = [num_comments]

        options['filter_comments'] = []
            
        for comments in num_comments:
            low = comments.get('range_min', None)
            high = comments.get('range_max', None)
            comment_filter = {}
        
            if low:
                comment_filter['min'] = low
            if high:
                comment_filter['max'] = high

            if comment_filter:
                options['filter_comments'].append(comment_filter)
            
        if not options['filter_comments']:
            del options['filter_comments']

    if price:
        if type(price) != list:
            price = [price]

        options['filter_price'] = filter(None, map(get_price_filter, price))
            

    if location:
        if type(location) != list:
            location = [location]
        options['filter_location'] = location

    if gender:
        if type(gender) != list:
            gender = [gender]

        options['filter_gender'] = set()

        for g in gender:
            if g.lower() == 'male':
                options['filter_gender'].add('m')
            elif g.lower() == 'female':
                options['filter_gender'].add('f')

        if options['filter_gender']:
            options['filter_gender'] = list(options['filter_gender'])
        else:
            del options['filter_gender']

    if social_followers:
        if type(social_followers) != list:
            social_followers = [social_followers]

        options['filter_followers'] = []
            
        for followers in social_followers:
            platform = followers.get('value', None)

            if platform:
                follower_filter = {}
                low = followers.get('range_min', None)
                high = followers.get('range_max', None)

                if low:
                    follower_filter['min'] = low
                if high:
                    follower_filter['max'] = high    

                if follower_filter:
                    follower_filter['platform'] = platform    
                    options['filter_followers'].append(follower_filter)
                    
        if not options['filter_followers']:
            del options['filter_followers']
            
    if social_activity:
        if type(social_activity) != list:
            social_activity = [social_activity]

        options['filter_activity'] = []

        for activity in social_activity:
            activity_filter = {}
            platform = activity.get('platform', None)
            level = activity.get('activity_level')
        
            if platform and level:
                activity_filter = {'platform': platform, 'level': level}

            if activity_filter:
                options['filter_activity'].append(activity_filter)
                
    if categories:
        options['filter_categories'] = categories

    if popularity and influencers:
        if type(popularity) != list:
            popularity = [popularity]

        options['filter_popularity'] = []
            
        for pop in popularity:
            pop_filter = get_popularity_filter(pop, influencers)

            if pop_filter:
                options['filter_popularity'].append(pop_filter)

        if not options['filter_popularity']:
            del options['filter_popularity']

    if influencer:
        options['filter_influencer_id'] = [influencer.id]
        
    '''
    Needs to be fixed to avoid breaking the 1024 limit in ES. 
    if influencer:
        options['filter_influencer_id'] = [influencer.id]
    elif influencers:
        options['filter_influencer_id'] = [i.id for i in influencers]
    '''

def keyword_search(options, esquery):
    ''' Prepares ES options for keyword searching.'''        
    keyword = esquery.get('keyword', [])
    mode = esquery.get('type', 'all')

    if mode in ('keyword', 'brand', 'name', 'blogname', 'blogurl',
                'all', 'location', 'hashtag', 'mention'):
        if mode == 'keyword':
            options['keyword_content'] = keyword
            options['keyword_title'] = keyword
        elif mode == 'brand':
            options['keyword_brand_name'] = set()
            options['keyword_brand_domain'] = set()
            
            for kw in keyword:
                brand = brand_from_keyword(kw)
                if brand:
                    options['keyword_brand_name'].add(brand.name)

                    if brand.domain_name:
                        options['keyword_brand_domain'].add(brand.domain_name)
            options['keyword_brand_name'] = list(options['keyword_brand_name'])
            options['keyword_brand_domain'] = list(options['keyword_brand_domain'])
        elif mode == 'name':
            options['keyword_name'] = keyword
        elif mode == 'blogname':
            options['keyword_blog_name'] = keyword
        elif mode == 'blogurl':
            options['keyword_blog_url'] = keyword
        elif mode == 'location':
            options['keyword_location'] = keyword
        elif mode == 'mention':
            mentions = [mention_from_keyword(kw, True) for kw in keyword]

            if mentions:
                options['keyword_content_mentions'] = mentions
                options['keyword_title_mentions'] = mentions

        elif mode == 'hashtag':
            hashtags = [hashtag_from_keyword(kw, True) for kw in keyword]

            if hashtags:
                options['keyword_content_hashtags'] = hashtags
                options['keyword_title_hashtags'] = hashtags
        elif mode == 'categories':
            options['keyword_categories'] = keyword
        elif mode == 'all':
            opt = {
                'keyword_blog_url': [],
                'keyword_blog_name': [],
                'keyword_content': [],
                'keyword_title': [],
                'keyword_location': [],
                'keyword_product_name': [],
                'keyword_designer_name': [],
                'keyword_content_hashtags': [],
                'keyword_title_hashtags': [],
                'keyword_content_mentions': [],
                'keyword_title_mentions': [],
                'keyword_brand': [],
                'keyword_brand_name': []
            }

            if keyword:
                for kw in keyword:
                    hashtag = hashtag_from_keyword(kw)
                    mention = mention_from_keyword(kw)

                    if not hashtag and not mention:
                        brand = brand_from_keyword(kw)
                        hashtag = hashtag_from_keyword(kw, True)
                        mention = mention_from_keyword(kw, True)

                        opt['keyword_blog_url'].append(kw)
                        opt['keyword_blog_name'].append(kw)
                        opt['keyword_content'].append(kw)
                        opt['keyword_title'].append(kw)
                        opt['keyword_location'].append(kw)
                        opt['keyword_product_name'].append(kw)
                        opt['keyword_designer_name'].append(kw)
                        opt['keyword_content_hashtags'].append(hashtag)
                        opt['keyword_content_mentions'].append(mention)
                        opt['keyword_title_hashtags'].append(hashtag)
                        opt['keyword_title_mentions'].append(mention)

                        if brand:
                            opt['keyword_brand'].append(brand)
                        else:
                            opt['keyword_brand_name'].append(kw)

                    elif hashtag:
                        opt['keyword_content_hashtags'].append(hashtag)
                        opt['keyword_title_hashtags'].append(hashtag)
                    elif mention:
                        opt['keyword_content_mentions'].append(mention)
                        opt['keyword_title_mentions'].append(mention)

            for column, terms in opt.iteritems():
                if 'column' in options and options[column]:
                    options[column] = set(options[column])
                    options[column].update(set(terms))
                    options[column] = list(options[column])
                else:
                    options[column] = terms
                    
def esquery_options(index_type, esquery, **kwargs):
    ''' esquery_options returns built options from the front end JSON esquery. '''    
    options = {}
    paginated_search(index_type, options, esquery)
    ordered_search(options, esquery)
    filtered_search(options, esquery, **kwargs)
    keyword_search(options, esquery)
    return options

def show_matches_only(search_query):
    ''' show_matches_only determines if matches are interleaved with non matching results. '''
    return search_query['display'] if 'display' in search_query else True
    
def filtered_keyword_search(index_type, indexed, esquery, **kwargs):
    ''' filtered_keyword_search performs a filtered keyword search against index_type. '''
    q, order, total = getattr(
        elastic_search_helpers, 'query_' + index_type + 's')(
            esquery_options(index_type, esquery, **kwargs),
            **kwargs
        )

    return indexed.filter(q), order, total

def filtered_influencer_defaults(esquery):
    ''' filtered_influnecer_defaults sets defaults for filtered influencer searching. '''
    if 'order_by' not in esquery:
        esquery['order_by'] = {}
        
    if not esquery['order_by']:
        esquery['order_by'] = {
            'field': 'popularity',
            'order': 'desc'
        }
        
def filtered_influencer_search(influencers, esquery):
    ''' filtered_influencer_search prepares options, defaults, and queries ES. '''
    filtered_influencer_defaults(esquery)
    return filtered_keyword_search('influencer', influencers, esquery, influencers=influencers)

def get_result_pagination(total_hits, offset, items_per_page):
    ''' get_result_pagination returns the total number of pages of results for the front end. '''
    return int(get_result_total(total_hits, offset) / items_per_page) + \
        int(get_result_total(total_hits, offset) % items_per_page > 0)

def get_result_total(total_hits, offset):
    ''' get_result_total returns the total number of hits for the query. '''
    return total_hits - offset
    
def next_page(search_query):
    ''' next_page returns the page number of the next set of results. '''
    return search_query['page'] + 1

def build_results(index_type, search_query, items_per_page, fetch, model, process):
    ''' build_result returns a full page of results, total, and pages. '''
    results = []
    count = 0
    offset = 0
    while len(results) < items_per_page:
        filtered_items, ordered_ids, total_hits = fetch(search_query)
        if not ordered_ids:
            break
        data = model(filtered_items)
        process(data, ordered_ids)

        ordered_results = search_results(data, ordered_ids)

        while len(results) < items_per_page and len(ordered_results) > 0:
            results.append(ordered_results.pop(0))

        ''' Uncomment to re-enable page scrubbing and offsetting. The break
            statement needs to be moved inside the first if statement. '''
        #if len(results) >= items_per_page or offset > total_hits:
        break
        #else:
            #count += 1
            #offset += (items_per_page - len(results)) + (items_per_page * count) + 1
            #search_query['_offset'] = offset

    #if offset:
    #    set_pagination_offset(index_type, search_query, offset, page=next_page(search_query))

    return (results, get_result_total(total_hits, offset),
                get_result_pagination(total_hits, offset, items_per_page))

def search_influencers_modeler(influencers, parameters=None, **kw):
    ''' search_influencers_serializer reads through cache, caching missing objects. '''
    from debra.models import Influencer
    from debra import serializers
    
    started = time.time()

    request = kw.get('request')

    brand = request.visitor["base_brand"] if request else None

    def get_cache_key(influencer_id):
        return 'influencer_%s_%i_%i' % (
            parameters.get('sub_tab', 'main_search'),
            influencer_id,
            int(bool(brand.flag_show_dummy_data if brand else False))
        )
    
    if settings.DEBUG:
        print 'Search influencer cache'
        
    cached = {}
    missed = []
    count = 0.0

    if parameters is None:
        parameters = {}

    for influencer_id in influencers.values_list('id', flat=True):
        influencer_data = mc_cache.get(get_cache_key(influencer_id))
        count += 1

        if influencer_data:
            cached[influencer_id] = influencer_data
        else:
            missed.append(influencer_id)

    if settings.DEBUG:
        print 'Search influencer cache miss %f%%' \
            % int(100 * len(missed) / count ) if count else 'No influencers to get from cache'

    if missed:
        if settings.DEBUG:
            print 'Search influencer cache getting missed influencers'

        missed_influencers = Influencer.objects.prefetch_related(
            'platform_set',
            'shelf_user__userprofile',
            'demographics_locality',
            # 'mails__candidate_mapping__job',
            # 'group_mapping__jobs__job',
        ).filter(id__in=missed)
        
        if settings.DEBUG:
            print 'Search influencer serializing cache misses', time.time() - started

        for influencer in missed_influencers:
            influencer.for_search = True
            influencer_data = serializers.InfluencerSerializer(
                influencer,
                context={
                    'sub_tab': parameters.get('sub_tab'),
                    'brand': kw.get('request').visitor["base_brand"] if kw.get('request') else None,
                    'request': kw.get('request'),
                }
            ).data
            mc_cache.set(get_cache_key(influencer.id), influencer_data)
            if settings.DEBUG:
                print 'Influencer # %s set to cache after %s sec' % (influencer.id, time.time() - started)
            cached[influencer.id] = influencer_data

    if settings.DEBUG:
        print 'Search influencers cache took ', time.time() - started

    return cached

def search_influencers_fetcher(searchable_influencers):
    ''' search_influencers_fetcher returns a fetcher for influencers that have been filtered. '''
    return lambda search_query: filtered_influencer_search(searchable_influencers, search_query)

def search_influencers_processor(influencer_data, ordered_ids):
    ''' search_influencers_processor does additional processing on influencers. '''
    for id, influencer in influencer_data.iteritems():
        influencer['details_url'] = reverse(
            'debra.search_views.blogger_info_json', args=(id,)
        )
        influencer['can_favorite'] = True
        
def search_influencers(search_query, items_per_page):
    ''' search_influencers searches influencers with a filtered keyword search. '''    
    # TODO: resolve the circular import models <--> search_helpers

    results, total, pages = build_results(
        'influencer',
        search_query,
        items_per_page,
        search_influencers_fetcher(get_searchable_influencers()),
        search_influencers_modeler,
        search_influencers_processor
    )

    return {
        'pages': pages,
        'results': results,
        'total_influencers': total,
        'slice_size': items_per_page
    }


def search_influencers_v3(search_query, items_per_page, **kw):
    """
    Here we retrieve influencers by options from ES and synchronize them with influencers from DB.
    :param search_query: raw query from request body, contains data from search form or saved query
    :param items_per_page: quantity of influencers per page to return
    :return: resulting dict of influencers' data, total number of influencers, total number of pages
    """
    from debra.serializers import (
        InfluencerElasticSearchSerializer,
        InfluencerElasticSearchSerializerV2,)

    # Retrieving page number for ES query
    try:
        page = int(search_query.get('page', 1))
        page -= 1
    except TypeError:
        page = 0

    # Building and calling ES query
    _, filtered_influencers_data, total = es_influencer_query_runner_v2(
        search_query, items_per_page, page, source=True, brand=kw.get('request').visitor["base_brand"] if kw.get('request') else None)

    offset = 0

    results = InfluencerElasticSearchSerializerV2(
    # results = InfluencerElasticSearchSerializer(
        filtered_influencers_data,
        context={
            'sub_tab': search_query.get('sub_tab'),
            'brand': kw.get('request').visitor["base_brand"] if kw.get('request') else None,
            'request': kw.get('request'),
        },
        many=True
    ).data

    result_total = get_result_total(total, offset)
    result_pages = get_result_pagination(total, offset, items_per_page)

    return {
        'pages': result_pages,
        'results': results,
        'total_influencers': result_total,
        'slice_size': items_per_page
    }


def search_influencers_v2(search_query, items_per_page, **kw):
    """
    Here we retrieve influencers by options from ES and synchronize them with influencers from DB.
    :param search_query: raw query from request body, contains data from search form or saved query
    :param items_per_page: quantity of influencers per page to return
    :return: resulting dict of influencers' data, total number of influencers, total number of pages
    """
    from debra.models import Influencer

    # Retrieving page number for ES query
    try:
        page = int(search_query.get('page', 1))
        page -= 1
    except TypeError:
        page = 0

    # if search_query.get('filters'):
    #     tags_filter = map(int, search_query.get('filters').get('tags', []))
    # else:
    #     tags_filter = []

    # Building and calling ES query
    # TODO: change source param to True for getting influencer's data from ES.
    q, filtered_influencer_ids, total = es_influencer_query_runner_v2(search_query,
                                                                      items_per_page,
                                                                      page,
                                                                      source=False)

    # print('* q: %s' % q)
    # print('* filtered_influencer_ids: %s' % filtered_influencer_ids)
    # print('* total: %s' % total)

    results = []
    offset = 0

    # TODO: For now ES index does not contain limiting fields for influencers not to show,
    # so when corresponding index fields will be added, this needs to be refactored

    # some magic from previous code
    while len(results) < items_per_page:

        # refactoring will be from here -->
        influencers = Influencer.objects.filter(q)

        #has_tags = bool(search_query.get('filters', {}).get('tags', None))

        #if settings.DEBUG or has_tags:
        #    influencers2 = influencers.filter(show_on_search=True)
        #else:
            # we made sure that all influencer.show_on_search=True are
            # marked old_show_on_search=True at this point and these are
            # the only ones that will continue to show up on search while
            # we upgrade other influencers on show_on_search=True
        #    influencers2 = influencers.filter(old_show_on_search=True)

        # if tags_filter:
        #     print 'TAGS FILTER', tags_filter
        #     influencers = influencers.filter(
        #         group_mapping__group_id__in=tags_filter)

        influencers = influencers.filter(
            profile_pic_url__isnull=False,
            score_popularity_overall__isnull=False
        ).exclude(
            source__icontains='brands'
        )  # .exclude(
        #     blacklisted=True)
        # <-- to here

        data = search_influencers_modeler(influencers, parameters=search_query, **kw)
        # filtered_influencer_ids is unused here
        search_influencers_processor(data, filtered_influencer_ids)
        
        ordered_results = search_results(data, filtered_influencer_ids)
        while len(results) < items_per_page and len(ordered_results) > 0:
            results.append(ordered_results.pop(0))
        break

    result_total = get_result_total(total, offset)
    result_pages = get_result_pagination(total, offset, items_per_page)

    return {
        'pages': result_pages,
        'results': results,
        'total_influencers': result_total,
        'slice_size': items_per_page
    }


def set_mailed_to_influencer(infs_data, brand_id=None):
    ''' manages checking whether current brand sent emails for each influencer or not '''
    from debra.models import MailProxy
    if brand_id:
        _mp_ids = set(MailProxy.objects.filter(
            brand_id=brand_id,
            influencer_id__in=[x["id"] for x in infs_data]
        ).values_list('influencer_id', flat=True))
        for res in infs_data:
            res["is_sent_email"] = res["id"] in _mp_ids


def set_influencer_invited_to(infs_data, brand_id=None):
    ''' manages filling up a list of campaigns each influencer has been invited to '''
    from debra.models import InfluencerJobMapping
    if brand_id:
        _influencer_ids = [x["id"] for x in infs_data]
        _job_ids = InfluencerJobMapping.objects.filter(
            Q(mailbox__influencer_id__in=_influencer_ids) | \
            Q(mapping__mailbox__influencer_id__in=_influencer_ids),
            Q(mailbox__brand_id=brand_id) | \
            Q(mapping__group__owner_brand_id=brand_id)
        ).values_list(
            'mailbox__influencer_id',
            'mapping__mailbox__influencer_id',
            'job'
        )
        _d = defaultdict(set)
        for inf_id, job_id in ((j[0] or j[1], j[2]) for j in _job_ids):
            _d[inf_id].add(job_id)
        for res in infs_data:
            res["invited_to"] = list(_d[res["id"]])


def set_influencer_collections(infs_data, brand_id=None):
    ''' influencer_collections manages adding influencers to collections for a brand. '''
    ''' ik - influencer key, key in result dict by which we can get
        influencer details
    '''
    from debra.models import InfluencerGroupMapping
    if brand_id:
        # fill 'collections_in' list
        _influencer_ids = [x["id"] for x in infs_data]
        _igm_ids = InfluencerGroupMapping.objects.exclude(
            status=InfluencerGroupMapping.STATUS_REMOVED
        ).exclude(
            group__archived=True
        ).filter(
            group__owner_brand_id=brand_id,
            influencer_id__in=_influencer_ids,
        ).values_list('influencer_id', 'group_id', 'group__name')
        _d = defaultdict(set)
        for each in _igm_ids:
            _d[each[0]].add(each[1:])
        for res in infs_data:
            res["collections_in"] = dict(_d.get(res["id"], []))


def set_influencer_analytics_collections(infs_data, brand_id=None):
    from debra.models import InfluencerAnalytics
    if brand_id:
        _d = defaultdict(set)
        _influencer_ids = [x["id"] for x in infs_data]
        _iac_ids = InfluencerAnalytics.objects.filter(
            influencer_analytics_collection__roipredictionreport__creator_brand_id=brand_id,
            influencer_id__in=_influencer_ids,
        ).values_list('influencer', 'influencer_analytics_collection')
        for each in _iac_ids:
            _d[each[0]].add(each[1])
        for res in infs_data:
            res['influencer_analytics_collections_in'] = list(
                _d.get(res["id"], []))


def set_brand_notes(infs_data, brand_id=None, user_id=None):
    from debra.models import InfluencerBrandUserMapping
    if user_id:
        _d = dict(InfluencerBrandUserMapping.objects.filter(
            user_id=user_id, influencer_id__in=[res['id'] for res in infs_data]
        ).values_list('influencer', 'notes'))
        for res in infs_data:
            res['note'] = _d.get(res['id'])


def filtered_influencer_posts_defaults(esquery):
    esquery['order_by'] = {
            'field': 'create_date',
            'order': 'desc'
    }
    esquery['page'] = 1

def filtered_influencer_posts_search(influencer, posts, esquery):
    ''' filtered_influencer_posts_search searches posts by influncer id. '''
    filtered_influencer_posts_defaults(esquery)
    return filtered_keyword_search('post', posts, esquery,
                                   influencer=influencer, key='url',
                                   transform='influencer')
    
def search_influencer_posts_fetcher(influencer):
    ''' search_influencer_posts_fetcher returns a fetcher for influencer_id. '''
    from debra.models import Posts, Influencer

    # these are collected here to avoid multiple db queries.
    if type(influencer) == list:
        q = Q(influencer_id__in=influencer)
    else:
        q = Q(influencer_id=influencer.id)
    posts = Posts.objects.filter(q)
    # posts = Posts.objects.filter(influencer_id=influencer.id)
    return lambda search_query: filtered_influencer_posts_search(
        influencer,
        posts,
        search_query
    )

def should_highlight(search_query):
    ''' should_highlight returns if highlighting should be applied for a query. '''
    if 'filters' in search_query:
        for key, f in search_query['filters'].iteritems():
            if f and key in ('categories', 'brand'):
                return True
    if 'keyword' in search_query and search_query['keyword']:
        return True
    return False

def search_influencer_posts_modeler(influencer, search_query, posts_per_page):
    ''' search_influencer_posts_influencers serializes returned post data. '''
    from debra.models import Posts, Platform
    from debra import serializers



    
    def _model(posts, post_urls, _posts):
        influencer_posts = {}
        start = 0

        while len(influencer_posts) < posts_per_page and start < len(_posts):
            end = posts_per_page if len(_posts) - start >= posts_per_page else len(_posts) - start

            serialized = serializePostsData(
                influencer,
                _posts[start:end],
                length_limit=SEARCH_FLYOUT_POST_LENGTH_LIMIT
            )
            
            for post in serialized:
                if post['url'] in post_urls:
                    post['highlight'] = True
                    post_urls.remove(post['url'])

                influencer_posts[post['url']] = post

                if len(influencer_posts) >= posts_per_page:
                    break

            start += end
                
        return influencer_posts

    def model_all_posts():
        blog_platforms = Platform.objects.filter(
            platform_name__in=Platform.BLOG_PLATFORMS + ['Tumblr'], influencer=influencer
        )
        
        _posts =  Posts.objects.filter(
            influencer=influencer,
            platform__in=blog_platforms,
            create_date__lte=datetime.date.today()
        ).order_by('-create_date').distinct('create_date')

        def model(posts):
            if should_highlight(search_query):
                post_urls = list(posts.values_list('url', flat=True))
            else:
                post_urls = []

            return _model(posts, post_urls, _posts)
        return model

    def model_matched_posts():
        def model(posts):    
            return _model(posts,
                    [p.url for p in posts[:]] if should_highlight(search_query) else [],
                    posts)
        return model
    return model_matched_posts() if show_matches_only(search_query) else model_all_posts()

def search_influencer_posts_processor(posts, ordered_urls):
    ''' search_influencer_posts_processor is the hook after modeler. '''
    while len(ordered_urls):
        ordered_urls.pop()
 
    ordered = []
    unordered = []

    for post in [post for (key, post) in posts.iteritems()]:
        if 'create_date' in post:
            ordered.append(post)
        else:
            unordered.append(post)

    ordered_urls.extend([post['url'] for post in sorted(ordered,
        key=lambda post: post['create_date'], reverse=True)] + [
            post['url'] for post in unordered])

                
def search_influencer_posts(influencer, search_query, items_per_page):
    ''' search_influencer_posts searches posts for a given influencer. '''
    search_query['count'] = items_per_page

    results, total, pages = build_results(
        'influencer_post',
        search_query,
        items_per_page,
        search_influencer_posts_fetcher(influencer),
        search_influencer_posts_modeler(influencer, search_query, items_per_page),
        search_influencer_posts_processor
    )

    return results

def filtered_influencer_products_defaults(esquery):
    ''' filtered_influencer_products_defaults sets defaults for influencer product queries. '''
    esquery['order_by'] = {
        'field': 'insert_date'
    }
    esquery['page'] = 1
    
def filtered_influencer_products_search(influencer, products, esquery):
    ''' filtered_posts_items_search searches items by post. '''
    filtered_influencer_products_defaults(esquery)
    return filtered_keyword_search('product', products, esquery,
                                   influencer=influencer, key='name',
                                   q='product_model__name',
                                   transform='influencer')

def search_influencer_products_fetcher(influencer):
    ''' search_influencer_items_fetcher returns a fetcher for influencer products. '''
    from debra.models import ProductModelShelfMap

    products = ProductModelShelfMap.objects.filter(
        influencer__id=influencer.id)

    return lambda search_query: filtered_influencer_products_search(
        influencer,
        products,
        search_query
    )

def search_influencer_products_modeler(influencer, search_query, items_per_page):
    ''' search_influencer_items_models returns a modelling method for products. '''
    from debra.models import ProductModelShelfMap
    
    def _model(items, item_names, _items):
        # filtered names from ES
        serialized = {}
        start = 0

        while len(serialized) < items_per_page and start < len(_items):
            end = items_per_page if len(_items) - start >= items_per_page else len(_items) - start

            things = serializeItemsData(_items[start:end])
            for item in things:
                if item['name'] in item_names:
                    item['highlight'] = True
                    item_names.remove(item['name'])

                serialized[item['name']] = item
                
                if len(serialized) >= items_per_page:
                    break

            start += end
                
        return serialized

    def model_all_items():
        _items = ProductModelShelfMap.objects.filter(
            influencer=influencer,
            img_url_feed_view__isnull=False
        ).order_by('-added_datetime').distinct('added_datetime')

        def model(items):
            if should_highlight(search_query):
                item_names = list(_items.values_list('product_model__name', flat=True))
            else:
                item_names = []
            return _model(items, item_names, _items)
        return model

    def model_matched_items():
        def model(items):
            return _model(items, [], items)
        return model
    return model_matched_items() if show_matches_only(search_query) else model_all_items()

def search_influencer_products_processor(products, ordered_names):
    ''' search_posts_items_processor is called after modeler. '''
    while len(ordered_names) > 0:
        ordered_names.pop()
    for name, product in products.iteritems():
        ordered_names.append(name)

def search_influencer_products(influencer, search_query, items_per_page):
    ''' search_influencer_items searches products for a given influencer. '''
    search_query['count'] = items_per_page

    results, total, pages = build_results(
        'influencer_product',
        search_query,
        items_per_page,
        search_influencer_products_fetcher(influencer),
        search_influencer_products_modeler(influencer, search_query, items_per_page),
        search_influencer_products_processor
    )
    return results
    
def query_from_request(request, source=None):

    data = copy(source)

    if data is None:
        try:
            request.body, request.GET
        except AttributeError:
            return {}
        else:
            try:
                data = json.loads(request.body)
            except ValueError:
                data = copy(request.GET)

    if 'and_or_filter_on' not in data:
        try:
            data['and_or_filter_on'] = request.visitor["base_brand"].flag_and_or_filter_on
        except AttributeError:
            pass
    
    return data

def filter_blogger_results(search_query, influencers):
    """
    filter the blogger results, this method is used by both blogger_search and followed_bloggers
    @param influencers - the influencers to filter based on the filters in the request
    @return QuerySet of Influencer filtered by the filters in the request
    """
    from debra.models import Brands
    import time

    score_mapping = None
    from django.core.cache import get_cache
    cache = get_cache('memcached')

    filters = search_query.get('filters')
    if filters is None:
        filters = {}
    elif not type(filters) is dict:
        filters = json.loads(filters)

    one_third_popularity = cache.get('one_third_popularity')
    two_thirds_popularity = cache.get('two_thirds_popularity')
    if one_third_popularity is None or two_thirds_popularity is None:
        count = influencers.count()
        one_third = count / 3
        two_thirds = 2 * count / 3
        influencers_popularity_sorted = influencers.order_by(
            'score_popularity_overall')
        one_third_popularity = influencers_popularity_sorted[
            one_third].score_popularity_overall
        two_thirds_popularity = influencers_popularity_sorted[
            two_thirds].score_popularity_overall
        cache.set('one_third_popularity', one_third_popularity)
        cache.set('two_thirds_popularity', two_thirds_popularity)

    overall_options = {}

    # basic plan
    if 'engagement' in filters and filters['engagement']:
        try:
            range_min = int(filters['engagement'].get("range_min", ""))
        except:  # ValueError, TypeError:
            range_min = None
        try:
            range_max = int(filters['engagement'].get("range_max", ""))
        except:  # ValueError, TypeError:
            range_max = None

        options = {
            "filter_blogger_engagement_range": {
                "min": range_min,
                "max": range_max
            }
        }

        overall_options.update(options)

    if 'popularity' in filters and filters['popularity']:

        pop_list = []

        if "Small" in filters["popularity"]:
            pop_list.append({
                "min": None,
                "max": one_third_popularity
            })

        if "Medium" in filters["popularity"]:
            pop_list.append({
                "min": one_third_popularity,
                "max": two_thirds_popularity
            })

        if "Large" in filters["popularity"]:
            pop_list.append({
                "min": two_thirds_popularity,
                "max": None
            })

        options = {
            "filter_blogger_popularity_range": pop_list
        }

        overall_options.update(options)

    if 'social' in filters and filters['social']:
        value = filters['social']["value"]
        try:
            range_min = int(filters['social'].get("range_min", ""))
        except (ValueError, TypeError):
            range_min = None
        try:
            range_max = int(filters['social'].get("range_max", ""))
        except (ValueError, TypeError):
            range_max = None

        options = {}
        options["filter_blogger_social_platform_num_followers"] = {
            "value": value,
            "min": range_min,
            "max": range_max
        }

        overall_options.update(options)

    if 'activity' in filters and filters['activity']:
        platform = filters['activity']['platform']
        level = filters['activity']['activity_level']
        options = {
            "filter_platform_activity_level": {
                "platform": platform,
                "level": level,
            }
        }
        overall_options.update(options)

    if 'categories' in filters and filters['categories']:
        overall_options['filter_categories'] = filters['categories']
        
    #  startup plan / trial
    if 'brand' in filters and filters['brand']:
        if not score_mapping:
            score_mapping = {}
        brands = []
        while filters['brand']:
            brand_domain = filters['brand'].pop()
            if type(brand_domain) == dict:
                brand_domain = brand_domain.get("value")
            brands += list(Brands.objects.filter(blacklisted=False, domain_name=brand_domain))

        options = {
            "filter_post_brand": brands
        }

        overall_options.update(options)

    if 'location' in filters and filters['location']:

        locations = filters['location']

        options = {
            'filter_location': locations
        }

        overall_options.update(options)

    if 'priceranges' in filters and filters['priceranges']:

        prices = filters['priceranges']
        price_mapped = []
        for price in prices:
            if price == "Cheap":
                price_mapped.append("cheap")
            elif price == "Mid-level":
                price_mapped.append("middle")
            else:
                price_mapped.append("expensive")

        options = {
            'filter_price_ranges': price_mapped
        }

        overall_options.update(options)

    if 'gender' in filters and filters['gender']:

        gender_list = []
        if "Male" in filters['gender']:
            gender_list.append('m')
        if "Female" in filters['gender']:
            gender_list.append('f')

        options = {
            "filter_blogger_gender": gender_list
        }

        overall_options.update(options)

    keyword = search_query.get('keyword')

    if keyword:
        if type(keyword) != list:
            keyword = [keyword]
            
    filter_type = search_query.get('type')
    sort_by = search_query.get('sort')
    order_by = get_order_by(search_query)

    if sort_by:
        sort = {}
        sort[sort_by] = {"order": "desc"}
        overall_options.update({
            'sort': sort
        })
    if order_by:
        if 'sort' in overall_options:
            del overall_options['sort']
            
        overall_options['order_by'] = order_by
    
    if keyword:
        t1 = time.time()
        if filter_type in ("keyword", "brand", "name", "blogname", "blogurl",
                           "all", "location", "hashtag", "mention"):
            options = None
            if filter_type == "keyword":
                options = {
                    "post_content_title": keyword,
                    "exact": True
                }
            elif filter_type == "brand":
                brands = [b for b in [brand_from_keyword(k) for k in keyword] if b is not None]

                if len(brands) is 0:
                    return influencers.filter(id=None), {}, 0
                else:
                    options = {
                        "post_brand": brands,
                        "exact": True
                    }
            elif filter_type == "name":
                options = {
                    "blogger_name": keyword
                }
            elif filter_type == "blogname":
                options = {
                    "blog_name": keyword
                }
            elif filter_type == "blogurl":
                options = {
                    "blog_url": keyword
                }
            elif filter_type == "location":
                options = {
                    "location": keyword
                }
            elif filter_type == "mention":
                options = {}
                mentions = [mention_from_keyword(kw, True) for kw in keyword]

                if mentions:
                    options["post_content_mentions"] = mentions
                    options["post_title_mentions"] = mentions

            elif filter_type == "hashtag":
                options = {}
                hashtags = [hashtag_from_keyword(kw, True) for kw in keyword]

                if hashtags:
                    options["post_content_hashtags"] = hashtags
                    options["post_title_hashtags"] = hashtags
                    
            elif filter_type == "categories":
                options = {
                    "post_categories": keyword
                }    
            elif filter_type == "all":
                options = {
                    "blog_url": [],
                    "blog_name": [],
                    "post_content_title": [],
                    "location": [],
                    "post_product_name": [],
                    "post_product_url": [],
                    "post_designer_name": [],
                    "post_content_hashtags": [],
                    "post_content_mentions": [],
                    "post_title_hashtags": [],
                    "post_title_mentions": [],
                    "post_brand": [],
                    "post_brand_name": []
                }
                
                for kw in keyword:
                    hashtag = hashtag_from_keyword(kw)
                    mention = mention_from_keyword(kw)

                    if hashtag is None and mention is None:
                        brand = brand_from_keyword(kw)
                        hashtag = hashtag_from_keyword(kw, True)
                        mention = mention_from_keyword(kw, True)

                        options["blog_url"].append(kw)
                        options["blog_name"].append(kw)
                        options["post_content_title"].append(kw)
                        options["location"].append(kw)
                        options["post_product_name"].append(kw)
                        options["post_product_url"].append(kw)
                        options["post_designer_name"].append(kw)

                    if hashtag:
                        options["post_content_hashtags"].append(hashtag)
                        options["post_title_hashtags"].append(hashtag)

                    if mention:
                        options["post_content_mentions"].append(mention)
                        options["post_title_mentions"].append(mention)

                        if brand:
                            options["post_brand"].append(brand)
                        else:
                            options["post_brand_name"].append(kw)
                
            if options:
                for opt in options.keys():
                    if not options[opt]:
                        del options[opt]
                        
                duplicates = ["post_brand", "location", "post_content_title"]
                
                for dup in duplicates:
                    try:
                        overall_options[dup] = single_to_plural(overall_options[dup])
                        options[dup] = single_to_plural(options[dup])
                        if overall_options[dup] and options[dup]:
                            overall_options[dup].extend(options[dup])
                            del options[dup]
                    except KeyError:
                        pass

                overall_options.update(options)
        t2 = time.time()
        log.info("keyword filtering finished in %f" % (t2 - t1))
        
    try:
        page_no = int(search_query.get("page", 1))
    except ValueError:
        page_no = 1

    overall_options["pagination"] = {
        "size": 60,
        "number": page_no - 1
    }

    q, sm, hits = elastic_search_helpers.es_influencer_query_runner(
        overall_options)

    influencers = influencers.filter(q)
    # order_by refers to ES, not Django
    if not order_by:
        influencers = influencers.order_by('-score_popularity_overall')
    
    score_mapping = sm

    return influencers, score_mapping, hits


def extract_brand_name(brand_name_url):
    """
    Try to find brand that matches given string.
    First we trying to search by domain, and if this fails, we just
    pick up first brand that contains given string in the name and has
    largest number of products related.
    @brand_name_url - string from the query
    @return - 'Brand' model instance, or None
    """
    from debra.models import Brands
    brand_name_url = brand_name_url.strip()
    if not brand_name_url:
        return
    domain = domain_from_url(brand_name_url)
    try:
        brand = Brands.objects.get(blacklisted=False, domain_name=domain)
    except:
        brand = Brands.objects.filter(
            blacklisted=False,
            name__iexact=brand_name_url
        ).order_by('-products_count')
        brand = brand[0] if brand else None
    return brand


def extract_brand_names(brand_name_urls):
    """
    Try to find brands that matches given strings.
    First we trying to search by domain, and if this fails, we just
    pick up first brand that contains given string in the name and has
    largest number of products related. -- remaking it to be done with a single query.
    @brand_name_url - string from the query
    @return - 'Brand' model instance, or None

    P.S. This function was rewritten due to problems with large quantities of urls to perform. Performing ~20 urls with
      it in a row took ~40 seconds of time which was unaffordable on production.
    """
    from debra.models import Brands

    # stripping urls, fetching their domains
    brand_name_urls = [url.strip() for url in brand_name_urls if len(url) > 0]
    if not brand_name_urls:
        return []

    brand_domains = [domain_from_url(url) for url in brand_name_urls if len(url) > 0]

    t1 = time.time()

    queries = [Q(domain_name=value) for value in brand_domains] + [Q(name=value.upper()) for value in brand_name_urls]

    query = queries.pop()

    for item in queries:
        query |= item

    brands = Brands.objects.exclude(blacklisted=True).filter(query)

    brands = list(brands)
    log.info('Time passed to get brands: %s ' % (time.time() - t1))
    return brands


def posts_keyword_options(esquery):
    options = {}
    if not 'keyword' in esquery or esquery['keyword'] is None:
        return options

    keywords = []
    hashtags = []
    mentions = []

    for kw in esquery['keyword']:
        if hashtag_from_keyword(kw) is not None:
            hashtags.append(kw)
        elif mention_from_keyword(kw) is not None:
            mentions.append(kw)
        else:
            keywords.append(kw)

    if keywords:
        options["post_brand_name"] = keywords
        options["filter_post_content_title"] = keywords
        options["product_name"] = keywords

    if hashtags:
        options["post_content_hashtags"] = hashtags
        options["post_title_hashtags"] = hashtags

    if mentions:
        options["post_content_mentions"] = mentions
        options["post_title_mentions"] = mentions

    return options


def posts_from_influencers(request, influencers, esquery, platform):
    # from debra import feeds_helpers
    query = json.loads(request.body)

    try:
        _, page = [(k, v) for k, v in query.items() if k.startswith('page')][0]
    except IndexError:
        page = None

    options = {
        "pagination": {
            "size": 50,
            "number": page - 1
        },
        "blogger_id": [x.id for x in influencers],
        "post_platform": platform
    }

    if not 'keyword' in esquery or esquery['keyword'] is None:
        options = None
    else:
        options.update(posts_keyword_options(esquery))

    q, sc, total_hits = elastic_search_helpers.es_post_query_runner(options)
    num_pages = int(
        math.ceil(float(total_hits) / options["pagination"]["size"]))
    return q or Q(), total_hits, num_pages


def influencers_by_search_parameters(parameters, page_size=20):
    """
    Constructs and executes ES query to retrieve influencers for page by parameters.
    """
    q, sc, total_hits = elastic_search_helpers.es_influencer_query_runner_v2(parameters,
                                                                             page_size)

    # num_pages = int(
    #     math.ceil(float(total_hits) / page_size))

    return q or Q(), sc, total_hits


def posts_for_influencers(parameters, page=0, page_size=50):
    """
    Constructs and executes ES query to retrieve posts for page by parameters.
    """
    post_ids, _, total_hits = elastic_search_helpers.es_post_query_runner_v2(parameters,
                                                                             page,
                                                                             page_size)

    num_pages = int(
        math.ceil(float(total_hits) / page_size))

    return post_ids, total_hits, num_pages


def products_for_influencers(parameters, page=0, page_size=50):
    """
    Constructs and executes ES query to retrieve products for page by parameters.
    """
    product_ids, _, total_hits = elastic_search_helpers.es_product_query_runner_v2(
        parameters, page, page_size)

    num_pages = int(math.ceil(float(total_hits) / page_size))

    return product_ids, total_hits, num_pages


def influencer_brands_filtering(influencer, brands_filter, esquery):
    options = {
        "pagination": {
            "size": 50
        },
        "blogger_id": influencer.id
    }

    brands = []
    
    def get_filters(runner):
        if options:
            f, _, _ = getattr(
            elastic_search_helpers, 'es_' + runner + '_query_runner')(options)
            return f
        return None

    def empty_keyword():
        if 'keyword' in esquery and esquery['keyword'] is not None:
            return False
        return True
    print "ESQUERY: %s" % esquery
    if 'filters' in esquery and 'brand' in esquery['filters']:
        brands = filter(None, map(extract_brand_name,
                    [b['value'].strip() for b in esquery['filters']['brand']]))

    if brands:
        options["post_brand"] = brands
    elif empty_keyword():
        options = None

    if not empty_keyword():
        keywords = []
        hashtags = []
        mentions = []

        for kw in esquery['keyword']:
            if hashtag_from_keyword(kw) is not None:
                hashtags.append(kw)
            elif mention_from_keyword(kw) is not None:
                mentions.append(kw)
            else:
                keywords.append(kw)

        if keywords:
            if not brands:
                options["post_brand_name"] = keywords

            options["filter_post_content_title"] = keywords
            options["product_name"] = keywords

        if hashtags:
            options["post_content_hashtags"] = hashtags
            options["post_title_hashtags"] = hashtags

        if mentions:
            options["post_content_mentions"] = mentions
            options["post_title_mentions"] = mentions

        if not keywords and not brands and not hashtags and not mentions:
            options = None
            
    return (get_filters('post') or Q()), \
        (get_filters('product') or Q()), \
        {brand.name.title().strip() for brand in brands}

def additional_posts_filtering(influencer, all_posts, posts_brands_filter_q, limit=12):
    from debra.models import Posts
    # we have to apply additional filters here
    if type(influencer) != list:
        influencer = [influencer]
    posts_qs = Posts.objects.filter(
        influencer__in=influencer,
        create_date__lte=datetime.date.today()
    ).select_related('influencer__shelf_user__userprofile')
    posts_qs = posts_qs.filter(posts_brands_filter_q)
    posts = posts_qs.order_by('-create_date').distinct('create_date')[:limit]
    # all_posts = data["posts"]
    related_posts = serializePostsData(None, posts, highlight=bool(posts_brands_filter_q))[:limit]
    uniques = set([x["url"] for x in related_posts])
    while len(related_posts) != limit and all_posts:
        post = all_posts.pop()
        if post["url"] in uniques:
            continue
        related_posts.append(post)
    # data["posts"] = related_posts
    return related_posts


def additional_items_filtering(influencer, all_items, items_brands_filter_q, brand_names):
    from debra.models import ProductModelShelfMap
    items = ProductModelShelfMap.objects.filter(
        post__influencer__id=influencer.id,
        img_url_feed_view__isnull=False
    ).filter(items_brands_filter_q)
    related_items = serializeItemsData(items, highlight=bool(items_brands_filter_q))[:25]
    uniques = set([x["img_url_feed_view"] for x in related_items])
    while len(related_items) != 12 and all_items:
        item = all_items.pop()
        if item["img_url_feed_view"] in uniques:
            continue
        related_items.append(item)
    for item in related_items:
        if item["brand"].title().strip() in brand_names:
            item["highlight"] = True
    return related_items


def get_influencer_posts(influencer, include_photos=False, long_post_content=False, include_sponsored_posts=False, limit=10):
    from debra.models import Posts, Platform
    from debra import serializers
    if type(influencer) != list:
        influencer = [influencer]
    # blog_platforms = Platform.objects.filter(
    #     platform_name__in=Platform.BLOG_PLATFORMS + ['Tumblr'], influencer__in=influencer)
    # posts_qs = Posts.objects.filter(
    #     influencer__in=influencer, platform__in=blog_platforms, create_date__lte=datetime.datetime.today())
    posts_qs = Posts.objects.filter(
        influencer__in=influencer,
        create_date__lte=datetime.datetime.today()
    ).select_related('influencer__shelf_user__userprofile')
    posts = posts_qs.order_by('-create_date').distinct('create_date')[:limit]

    data = {}

    data["posts"] = serializePostsData(
        None, posts, length_limit=long_post_content and 50 or 30)

    if include_sponsored_posts:
        posts_sponsored = posts_qs.filter(is_sponsored=True).order_by(
            '-create_date').distinct('create_date')[:limit]
        data["posts_sponsored"] = serializePostsData(
            influencer, posts_sponsored, length_limit=long_post_content and 50 or 30)

    if include_photos:
        posts = Posts.objects.filter(
            influencer__in=influencer, platform__platform_name="Instagram").exclude(platform__url_not_found=True).order_by('?')[:20]
        photos = []
        for post in posts:
            stripped_content, images = tagStripper(post.content)
            if not images:
                continue
            photos.append(images[0])
        data["photos"] = photos

    return data, posts_qs


def get_influencer_posts_v2(influencer_ids,
                            parameters,
                            page_size,
                            include_photos=False,
                            long_post_content=False,
                            include_sponsored_posts=False,
                            limit=10,
                            db_only=False,
                            **kw):
    """
    Returns data about posts by getting list of ids from ES and fetching posts data from DB.

    :param influencer_ids - Influencer object or list of them for whom posts are returned
    :param parameters - dict of parameters to form ES query.
    :param page_size - how many results are returned per page

    :param include_photos - flag to include photos
    :param long_post_content - flag of long post content (50 symbols)
    :param include_sponsored_posts - flag to include sponsored posts
    :param limit

    :returns dict of influencers posts/photos/sponsored posts, number of total posts
    """

    from debra.models import Posts, Influencer, Platform

    request = kw.get('request')
    brand = request.visitor["base_brand"]
    print("GOT BRAND: %r" % brand)
    influencer = None
    if not influencer_ids:
        influencer_ids = []
    elif isinstance(influencer_ids, Influencer):
        influencer = influencer_ids
        influencer_ids = [influencer.id, ]
    else:
        if type(influencer_ids) != list:
            influencer_ids = [influencer_ids, ]

        if len(influencer_ids) == 1:
            influencer = Influencer.objects.prefetch_related(
                'platform_set'
            ).get(id=influencer_ids[0])

    # storing influencer's id in parameters for query builder
    if influencer_ids:
        parameters['influencer_ids'] = influencer_ids

    # Retrieving page number for ES query
    try:
        page = int(parameters.get('page', 1))
        page -= 1
    except TypeError:
        page = 0

    if influencer is None:
        prefetch_list = ['platform_set']
    else:
        prefetch_list = []
        if influencer.has_artificial_blog_url and not parameters.get('post_platform'):
            parameters['post_platform'] = Platform.SOCIAL_PLATFORMS

    if db_only:
        db_params = dict(
            influencer_id__in=influencer_ids
        )
        if parameters.get('post_platform'):
            db_params['platform__platform_name__in'] = parameters['post_platform']
        highlighted_ids, total_posts = [], None
    else:
        # Getting list of post ids from ES depending on search parameters
        print("PASSING BRAND: %r" % brand)
        post_ids, highlighted_ids, total_posts = es_post_query_runner_v2(parameters,
                                                                         page,
                                                                         page_size,
                                                                         highlighted_first=True,
                                                                         brand=brand)
        # if highlighted_ids and len(highlighted_ids) > 0:
        #     db_params = dict(id__in=highlighted_ids)
        # else:
        #     db_params = dict(id__in=post_ids)
        db_params = dict(id__in=post_ids)

        if settings.DEBUG:
            print('* Post IDs: %s' % post_ids)
            print('* Highlighted Post IDS: %s' % highlighted_ids)
            print('* Total posts: %s' % total_posts)

    # getting posts from DB
    posts_qs = Posts.objects.filter(
        create_date__lte=datetime.datetime.today(),
        **db_params
    ).exclude(
        platform__url_not_found=True
    ).prefetch_related(
        'influencer__shelf_user__userprofile',
        *prefetch_list
    ).order_by('-create_date')

    # posts are ordered by decreasing create_date and are distinct by url
    posts_from_db = posts_qs.distinct('url', 'create_date')

    if db_only:
        posts_from_db = posts_from_db[:limit]
    else:
        # sorting posts by list of ids from ES
        posts_from_db = dict([(post.id, post) for post in posts_from_db])
        posts_from_db = [posts_from_db[post_id] for post_id in post_ids if post_id in posts_from_db]


    # serializing posts for result
    posts = serialize_posts_data_v2(influencer,
                                    posts_from_db,
                                    length_limit=(50 if long_post_content else 30),
                                    highlighted_ids=highlighted_ids,
                                    **kw
                                    )

    # adding sponsored posts if required
    posts_sponsored = []
    if include_sponsored_posts:
        posts_sponsored_from_db = posts_qs.filter(is_sponsored=True).order_by(
            '-create_date').distinct('url', 'create_date')[:limit]
        posts_sponsored = serialize_posts_data_v2(influencer,
                                                  posts_sponsored_from_db,
                                                  length_limit=(50 if long_post_content else 30),
                                                  highlighted_ids=highlighted_ids,
                                                  **kw)

    # adding photos if required
    photos = []
    if include_photos:
        posts_from_db = Posts.objects.filter(
            influencer__in=influencer_ids,
            platform__platform_name="Instagram").exclude(platform__url_not_found=True).order_by('?')[:20]
        for post in posts_from_db:
            stripped_content, images = tagStripper(post.content)
            if not images:
                continue
            photos.append(images[0])

    return posts, posts_sponsored, photos, total_posts


def get_influencer_products_v2(influencer_ids,
                               parameters,
                               page_size,
                               db_only=False,
                               limit=10):
    """
    Returns data about products by getting list of ids from ES and fetching items data from DB.

    ProductModelShelfMap is get by ProductModels fetched by ES returned ids and Influencers, defined by influencer_ids.

    :param influencer_ids - Influencer object or list of them for whom posts are returned
    :param parameters - dict of parameters to form ES query.
    :param page_size - how many results are returned per page

    :returns dict of influencers items posts, total of all items
    """
    from debra.models import ProductModelShelfMap

    # updating influencers to list and moving it to parameters for ES query builder
    if type(influencer_ids) != list:
        influencer_ids = [influencer_ids, ]

    # storing influencer's id in parameters for query builder
    if influencer_ids:
        parameters['influencer_ids'] = influencer_ids

    # Retrieving page number for ES query
    try:
        page = int(parameters.get('page', 1))
        page -= 1
    except TypeError:
        page = 0

    if db_only:
        db_params = {}
        highlighted_product_ids, total = [], None
    else:
        # Getting list of post ids from ES depending on search parameters
        product_ids, highlighted_product_ids, total = es_product_query_runner_v2(parameters,
                                                                                 page,
                                                                                 page_size,
                                                                                 highlighted_first=True)
        db_params = dict(product_model__id__in=product_ids,)
        if settings.DEBUG:
            print('* Item IDS: %s' % product_ids)
            print('* Highlighted item IDs: %s' % highlighted_product_ids)
            print('* Total items: %s' % total)

    items_from_db = ProductModelShelfMap.objects.filter(
        influencer__id__in=influencer_ids,
        img_url_feed_view__isnull=False,
        **db_params
    ).prefetch_related('product_model')  # .order_by('-product_model__insert_date')

    # list of ids for ProductModelShelfMap, not for product_model get from ES
    highlighted_items_ids = []

    if db_only:
        items = items_from_db[:limit]
    else:
        # sorting posts by list of ids from ES
        items = dict()
        for item in items_from_db:
            items[item.product_model.id] = item
            if item.product_model.id in highlighted_product_ids:
                highlighted_items_ids.append(item.id)

        items = [items[product_id] for product_id in product_ids if product_id in items]

    items_data = serialize_items_data_v2(items, highlighted_items_ids)

    return items_data, len(items_data)


def get_influencer_items(influencer, posts_qs):
    from debra.models import ProductModelShelfMap
    items = ProductModelShelfMap.objects.filter(
        post__in=posts_qs,
        img_url_feed_view__isnull=False,
    )
    profile_data = dict(
        items=serializeItemsData(items)[:40]
    )
    return profile_data


def get_popularity_stats(influencer, prefetched=False, debug=True):
    from debra.models import Platform
    from debra.serializers import PopularityTimeSeriesSerializer
    from debra.helpers import CacheQuerySet

    t0 = time.time()
    data = {}
    platform_to_key = lambda p: "%s_%i" % (p['platform_name'].lower(), p['id'])
    followers_platforms = set(
        ['Twitter', 'Facebook', 'Instagram', 'Pinterest', 'Youtube'])
    comments_platforms = set(Platform.BLOG_PLATFORMS)
    platforms = followers_platforms | comments_platforms

    pls = influencer.all_platforms.filter(
        lambda pl: pl['platform_name'] in platforms and\
            not pl.get('url_not_found')
    )

    time_series = influencer.all_time_series.filter(
        lambda ts: ts['snapshot_date'] and pls.safe_get(ts['platform']) and\
            pls.safe_get(ts['platform'])['platform_name'] in platforms)

    # if prefetched:
    #     time_series = [
    #         ts for ts in influencer.popularitytimeseries_set.all()
    #         if ts.snapshot_date and ts.platform.platform_name in platforms and
    #             not ts.platform.url_not_found]
    # else:
    #     time_series = list(influencer.popularitytimeseries_set.select_related(
    #         'platform'
    #     ).filter(
    #         snapshot_date__isnull=False, platform__platform_name__in=platforms,
    #         platform__url_not_found=False
    #     ))

    if debug:
        print 'Time series query', time.time() - t0
    popularity_stats = {
        'followers_data': [],
        'comments_data': [],
        'series': []
    }
    platform_name_counts = {}

    for platform in pls.get():
        pl_name = platform['platform_name']
        if pl_name not in platform_name_counts:
            platform_name_counts[pl_name] = 0
        platform_name_counts[pl_name] += 1
        series_data = {
            # 'label': "%s [%i]" % (pl_name, platform_name_counts[pl_name]),
            'label': "%s" % (pl_name,),
            'key': platform_to_key(platform),
            'comments': pl_name in comments_platforms,
            'followers': pl_name in followers_platforms,
        }
        popularity_stats['series'].append(series_data)

    group_by_date = {}
    for stamp in time_series.get():
        date = stamp['snapshot_date'].date()
        if date not in group_by_date:
            group_by_date[date] = []
        group_by_date[date].append(stamp)
    for date, stamps in group_by_date.iteritems():
        json_data = {
            'date': date,
        }
        followers_sum = 0
        for stamp in stamps:
            if not pls.safe_get(stamp['platform']) or pls.safe_get(
                    stamp['platform'])['platform_name'] not in followers_platforms:
                continue
            p_name = platform_to_key(pls.get(stamp['platform']))
            json_data['%s_num_followers' % p_name] = stamp['num_followers']
            followers_sum += stamp['num_followers'] or 0
        if followers_sum > 0:
            popularity_stats['followers_data'].append(json_data)
        json_data = {
            'date': date,
        }
        comments_sum = 0
        for stamp in stamps:
            if not pls.safe_get(stamp['platform']) or pls.get(
                    stamp['platform'])['platform_name'] not in comments_platforms:
                continue
            p_name = platform_to_key(pls.get(stamp['platform']))
            json_data['%s_num_comments' % p_name] = stamp['num_comments']
            comments_sum += stamp['num_comments'] or 0
        if comments_sum > 0:
            popularity_stats['comments_data'].append(json_data)
    popularity_stats["comments_data"].sort(key=lambda x: x['date'])
    popularity_stats["followers_data"].sort(key=lambda x: x['date'])
    followers_date2index = {}
    comments_date2index = {}
    for n, p in enumerate(popularity_stats["followers_data"]):
        followers_date2index[p["date"]] = n
    for n, p in enumerate(popularity_stats["comments_data"]):
        comments_date2index[p["date"]] = n

    def find_prev_value_comments(name, c_date):
        idx = comments_date2index[c_date] - 1
        while idx >= 0:
            if popularity_stats["comments_data"][idx][name]:
                return popularity_stats["comments_data"][idx][name]
            idx -= 1
        return 0

    def find_prev_value_followers(name, c_date):
        idx = followers_date2index[c_date] - 1
        while idx >= 0:
            if popularity_stats["followers_data"][idx][name]:
                return popularity_stats["followers_data"][idx][name]
            idx -= 1
        return 0

    platform_sums = {}
    for json_data in popularity_stats["followers_data"]:
        for series in popularity_stats['series']:
            if not series["followers"]:
                continue
            p_name = series["key"]
            if json_data.get('%s_num_followers' % p_name) is None:
                json_data['%s_num_followers' % p_name] = find_prev_value_followers(
                    '%s_num_followers' % p_name, json_data["date"])

            if p_name not in platform_sums:
                platform_sums[p_name] = {
                    'followers': 0,
                }
            platform_sums[p_name]['followers'] += json_data.get(
                '%s_num_followers' % p_name, 0) or 0
    for json_data in popularity_stats["comments_data"]:
        for series in popularity_stats['series']:
            if not series["comments"]:
                continue
            p_name = series["key"]
            if json_data.get('%s_num_comments' % p_name) is None:
                json_data['%s_num_comments' % p_name] = find_prev_value_comments(
                    '%s_num_comments' % p_name, json_data["date"])

            if p_name not in platform_sums:
                platform_sums[p_name] = {
                    'comments': 0,
                }
            platform_sums[p_name]['comments'] += json_data.get(
                '%s_num_comments' % p_name, 0) or 0

    data["popularity_stats"] = popularity_stats
    data["popularity_sums"] = platform_sums
    print '* Stats took {} to load'.format(time.time() - t0)
    return data


def get_brand_mentions(influencer, exclude_itself=False):
    from debra.models import BrandMentions
    from debra.templatetags.custom_filters import remove_dot_com
    from xpathscraper import utils
    data = {}
    t0 = time.time()
    brand_mentions = list(BrandMentions.objects.select_related('brand').filter(
        influencer=influencer, brand__blacklisted=False, brand__date_edited__isnull=False).order_by('-count_notsponsored'))
    print 'Brand mentions query', time.time() - t0
    mentions_sponsored = []
    mentions_notsponsored = []
    for mention in brand_mentions:
        if exclude_itself and influencer.blog_url and\
                utils.domain_from_url(influencer.blog_url) == mention.brand.domain_name:
            continue
        if mention.brand.blacklisted is False and mention.count_sponsored > 0:
            mention_data = {
                "name": remove_dot_com(mention.brand.name),
                "count": mention.count_sponsored,
                "domain_name": mention.brand.domain_name,
            }
            mentions_sponsored.append(mention_data)
        if mention.brand.blacklisted is False and mention.count_notsponsored > 0:
            mention_data = {
                "name": remove_dot_com(mention.brand.name),
                "count": mention.count_notsponsored,
                "domain_name": mention.brand.domain_name,
            }
            mentions_notsponsored.append(mention_data)
    data["mentions_sponsored"] = mentions_sponsored
    data["mentions_notsponsored"] = mentions_notsponsored

    return data


def additional_brand_mentions_filtering(mentions_notsponsored, brand_names):
    # mentions reordering and normalize
    mentions_reordered = []
    mentions_reordered_uniques = set()
    for mention in mentions_notsponsored:
        name = mention["name"].title().strip()
        if name in mentions_reordered_uniques:
            [x for x in mentions_reordered if x["name"] == name][0]["count"] += mention["count"]
            continue
        mentions_reordered_uniques.add(name)
        mention["name"] = name
        if name in brand_names:
            mention["highlight"] = True
            mentions_reordered.insert(0, mention)
        else:
            mentions_reordered.append(mention)
    # data["mentions_notsponsored"] = mentions_reordered
    return mentions_reordered


def get_influencer_json(influencer, **kw):
    """
    serialize influencer information into jsonizable dictionary
    """

    # local imports to prevent circular deps
    from debra import serializers

    include_photos = kw.get('include_photos', False)
    long_post_content = kw.get('long_post_content', False)
    include_sponsored_posts = kw.get('include_sponsored_posts', False)
    with_posts = kw.get('with_posts', True)
    with_items = kw.get('with_items', True)
    with_stats = kw.get('with_stats', True)
    with_age_dist = kw.get('with_age_dist', True)
    with_brand_mentions = kw.get('with_brand_mentions', True)
    parameters = kw.get('parameters')
    request = kw.get('request')

    if parameters is None:
        parameters = {}

    # ======================= populate influencer =============================
    t0 = time.time()
    serializer = serializers.InfluencerSerializer(influencer, context={
        'sub_tab': parameters.get('sub_tab'),
        'description_length_limit': long_post_content and 200 or 140,
        'brand': request.visitor["base_brand"] if request else None,
    })
    data = serializer.data
    print 'InfluencerSerializer', time.time() - t0

    # TODO: get rid of these extra fields, they are just duplicates,
    # but 'blog_name' (not 'blogname') might be used somewhere on the
    # front-end
    data["profile_pic"] = influencer.profile_pic
    data["blog_name"] = data["blogname"]


    # populate profile data / items
    if influencer.shelf_user and influencer.shelf_user.userprofile:
        profile = influencer.shelf_user.userprofile
    else:
        profile = None

    profile_data = get_social_data(influencer=influencer, profile=profile, **kw)
    profile_data["aboutme"] = influencer.description
    profile_data["items_page"] = influencer.items_page
    profile_data["posts_page"] = influencer.posts_page
    profile_data["posts_sponsored_page"] = influencer.posts_sponsored_page
    profile_data["about_page"] = influencer.about_page

    profile_data["photos_page"] = influencer.photos_page
    profile_data["tweets_page"] = influencer.tweets_page
    profile_data["pins_page"] = influencer.pins_page
    profile_data["videos_page"] = influencer.youtube_page

    social_pages = ["photos_page", "tweets_page", "pins_page",
        "videos_page"]
    try:
        any_social_page = profile_data.get(
            [p for p in social_pages if profile_data.get(p)][0])
    except IndexError:
        any_social_page = None
    profile_data["any_social_page"] = any_social_page

    # ======================= populate posts ==================================

    if with_posts:
        # include only blog posts
        posts_params = {
            'post_platform': ['Blogspot', 'Wordpress', 'Custom', 'Tumblr']
        }
        posts, posts_sponsored, photos, total_posts = get_influencer_posts_v2(
            influencer,
            parameters=posts_params,
            page_size=10,
            include_photos=include_photos,
            long_post_content=long_post_content,
            db_only=True,
            include_sponsored_posts=include_sponsored_posts,
            request=request
        )
        if posts:
            data['posts'] = posts
        if posts_sponsored:
            data['posts_sponsored'] = posts_sponsored
        if photos:
            data['photos'] = photos
        data['total_posts'] = total_posts

    # ======================= populate items ==================================


    if with_items:
        items, total_items = get_influencer_products_v2(
            influencer_ids=influencer.id,
            parameters={},
            page_size=10,
        )
        if items:
            profile_data['items'] = items
        profile_data['total_items'] = total_items


    # if with_items and with_posts:
    #     profile_data.update(get_influencer_items_v2(influencer, posts_qs))

    data["profile"] = profile_data

    # ======================== populate brand mentions ========================
    if with_brand_mentions:
        data.update(get_brand_mentions(influencer))

    # ======================= popularity stats ================================

    if with_stats:
        data.update(get_popularity_stats(influencer))

    # ======================= viewers distributions ================================

    if with_age_dist:
        items = [(k, getattr(influencer, k))
            for k in influencer._meta.get_all_field_names()
            if k.startswith('dist_age_')]
        data['age_distribution'] = [{
            'label': '{}{}'.format(
                k[9:].replace('_', '-'), '+' if not '_' in k[9:] else ''),
            'value': v,
        } for k, v in items if v]

    return data


def search_influencers_old(search_query, items_per_page):
    # TODO: resolve the circular import models <--> search_helpers
    from debra.models import Influencer
    from debra import serializers

    print "Preflight",

    preflight_influencers = Influencer.objects.only('id')  # .select_related(*preflight_select_related)
    # preflight_influencers = preflight_influencers.prefetch_related(*preflight_prefetch_related)

    # Temporary fix to make sure new activity_levels do not impact our production search results
    if settings.DEBUG:
        preflight_influencers = preflight_influencers.filter(show_on_search=True, profile_pic_url__isnull=False).exclude(blacklisted=True)
    else:
        # we made sure that all influencer.show_on_search=True are marked old_show_on_search=True at this point
        # and these are the only ones that will continue to show up on search while we upgrade other influencers on
        # show_on_search=True
        preflight_influencers = preflight_influencers.filter(
            old_show_on_search=True, profile_pic_url__isnull=False).exclude(blacklisted=True)
    preflight_influencers = preflight_influencers.filter(score_popularity_overall__isnull=False)
    preflight_influencers = preflight_influencers.order_by('-score_popularity_overall')
    preflight_influencers = preflight_influencers.exclude(source='brands')
    #preflight_influencers = preflight_influencers.order_by('-score_engagement_overall')
    preflight_influencers = preflight_influencers.distinct()

    print "Filtering",

    filtered_influencers, score_mapping, total_hits = filter_blogger_results(search_query, preflight_influencers)
    print "Pagination",

    influencer_ids = list(filtered_influencers.values_list('id', flat=True))
    influencer_ids.sort(key=lambda x: score_mapping.get(x, 0), reverse=True)

    data = {
        'pages': int(total_hits / items_per_page) + int(total_hits % items_per_page > 0),
        'results': []
    }

    print "Cache",

    cache_misses = []
    cnt = 0.0
    if score_mapping:
        for iid in influencer_ids:
            influencer_data = cache.get("influencer_search_%i" % iid)
            cnt += 1
            if influencer_data:
                data["results"].append(influencer_data)
            else:
                cache_misses.append(iid)

    if cnt:
        print "cache miss %f%%" % int(100 * len(cache_misses) / cnt)
    if cache_misses:
        print "Fetching influencers not in cache.",
        fetched_influencers = Influencer.objects
        prefetch_influencer_data = [
            'platform_set',
            'shelf_user__userprofile',
            'shelf_user',
            'demographics_locality',
            'mails__candidate_mapping__job',
            'group_mapping__jobs__job'
        ]
        fetched_influencers = fetched_influencers.prefetch_related(*prefetch_influencer_data)
        object_list = fetched_influencers.filter(id__in=cache_misses)
        if score_mapping:
            object_list = list(object_list)
            object_list.sort(key=lambda x: score_mapping.get(x.id, 0), reverse=True)
        else:
            fetched_influencers = object_list.order_by('-score_popularity_overall')
            object_list = fetched_influencers
        print "Serialization",

        for influencer in object_list:
            influencer.for_search = True  # Pass a hint to the serializer, and avoid some DB fetches
            influencer_data = serializers.InfluencerSerializer(influencer).data
            cache.set("influencer_search_%i" % influencer.id, influencer_data)
            data["results"].append(influencer_data)

    print "Postprocessing",

    for influencer in data["results"]:
        influencer["details_url"] = reverse('debra.search_views.blogger_info_json', args=(influencer["id"],))
        influencer["can_favorite"] = True
        # if base_brand and base_brand.is_subscribed and base_brand.stripe_plan in STRIPE_COLLECTION_PLANS:
        #     influencer["is_favoriting"] = influencer["id"] in faved_influencers
        # else:
        #     influencer["can_favorite"] = False

    data["total_influencers"] = total_hits
    data["slice_size"] = 60
    return data


def get_brand_groups_list(brand, base_brand):
    from debra.serializers import unescape
    groups_list = brand.influencer_groups.exclude(
        archived=True
    ).filter(
        creator_brand=base_brand
    ).order_by(
        'name'
    ).only('name', 'id')

    groups_list = [{'id': g.id, 'name': unescape(g.name)} for g in groups_list]

    return groups_list


def get_brand_saved_queries_list(base_brand):
    from debra.serializers import unescape
    saved_queries = base_brand.saved_queries.exclude(
        name__isnull=True
    ).exclude(
        archived=True
    ).order_by(
        'name',
    ).only('id', 'name')

    saved_queries = [{
        'id': q.id, 'name': unescape(q.name)} for q in saved_queries]

    return saved_queries


def get_brand_tags_list(brand, base_brand):
    from debra.serializers import unescape
    tags = brand.influencer_groups.exclude(
        archived=True
    ).filter(
        creator_brand=brand,
        system_collection=False
    ).order_by(
        'name'
    ).only('id', 'name')

    tags = [{'id': t.id, 'name': unescape(t.name)} for t in tags]

    return tags


def sorting_options(sort_params, serializer_class,
    annotation_fields=None, default_params=None, hidden_fields=None):

    if annotation_fields is None:
        annotation_fields = {}

    sort_by_number, sort_direction = sort_params

    def _undir_field(f):
        return f[1:] if f.startswith('-') else f

    def _dir_field(f, d):
        return '{}{}'.format('-' if d else '', _undir_field(f))

    def _get_dir_field(s, f):
        d = f.startswith('-')
        res = s.get(_undir_field(f), f)
        if type(res) is tuple:
            return (_dir_field(res[0], d),) + res[1:]
        return _dir_field(res, d)

    def _sort_by_fields(sort_by_number, sort_direction, default=None):

        _items = serializer_class().get_visible_items(hidden_fields)
        _fields = map(lambda x: x[1].source or x[0], _items)
        _sort_by_field = dict(enumerate(_fields, start=1)).get(sort_by_number)

        if _sort_by_field is not None:
            _sort_by_field = _dir_field(_sort_by_field, sort_direction)
            # check for complex fields
            _sort_by_field = _get_dir_field(
                dict(serializer_class.SORT_BY_FIELDS), _sort_by_field)

        if type(_sort_by_field) is tuple:
            _sort_by_field = list(_sort_by_field)
        else:
            _sort_by_field = [_sort_by_field]

        if default is None:
            default = [('modified', 1)]
        _sort_by_field.extend([_dir_field(s, d) for s, d in default])

        _sort_by_field = filter(None, _sort_by_field)

        _sort_by_field = ['__'.join(x.split('.')) for x in _sort_by_field]

        _sort_by_field = [_get_dir_field(
            annotation_fields, x) for x in _sort_by_field]

        print "ORDER BY:", _sort_by_field

        return tuple(_sort_by_field)

    return _sort_by_fields(sort_by_number, sort_direction, default_params)


def generic_reporting_table_context(
    request,
    queryset=None,
    serializer_class=None,
    serializer_context=None,
    annotation_fields=None,
    order_params=None,
    default_order_params=None,
    distinct=None,
    include_total=True,
    total_with_fields=False,
    limit=None,
    hidden_fields=None,
    visible_columns=None,
    pre_serialize_processor=None,
    queryset_extra=None,
    shelf_user=None):

    from debra.serializers import serialize_post_analytics_data, count_totals
    from debra.helpers import paginate

    order_params = order_params or []
    annotation_fields = annotation_fields or {}
    serializer_context = serializer_context or {}

    serializer_context.update({
        'brand': request.visitor["base_brand"],
        'request': request,
    })

    page = int(request.GET.get('page', 1))
    sort_by = int(request.GET.get('sort_by', 0))
    sort_direction = int(request.GET.get('sort_direction', 0))
    paginate_by = int(request.GET.get('paginate_by', 30))

    order_params.extend(sorting_options(
        (sort_by, sort_direction), serializer_class, annotation_fields,
        default_params=default_order_params, hidden_fields=hidden_fields,
    ))

    print '* collection ordering: {}'.format(order_params)

    # extended_order_params = []
    # queryset_extra = copy(queryset_extra)
    # for order_param in order_params:
    #     extra_param = 'agr_notnull_{}'.format(order_param.strip('-'))
    #     queryset_extra[extra_param] = '{} IS NOT NULL'.format(order_param)
    #     extended_order_params.append(order_param)
    #     extended_order_params.append('-{}'.format(extra_param))

    queryset = queryset.order_by(*order_params)
    if distinct:
        queryset = queryset.distinct(*distinct)
    if limit:
        queryset = queryset[:limit]

    paginated_queryset = paginate(
        queryset,
        page=page,
        paginate_by=paginate_by,
    )

    if pre_serialize_processor:
        pre_serialize_processor(paginated_queryset)

    serialized_data = serialize_post_analytics_data(
        paginated_queryset, serializer_class,
        serializer_context=serializer_context
    )

    if shelf_user is None:
        try:
            shelf_user = request.user.userprofile
        except AttributeError:
            shelf_user = None
 
    context = {
        'sort_by': sort_by,
        'sort_direction': sort_direction,
        'search_page': True,
        'type': 'followed',
        'selected_tab': 'competitors',
        'shelf_user': shelf_user,
        'request': request,
    }

    hidden_fields = hidden_fields or []
    hidden_fields = list(set(hidden_fields + serializer_class.HIDDEN_FIELDS))

    fields = serializer_class.get_visible_fields(hidden_fields)
    headers = serializer_class.get_headers(hidden_fields, visible_columns)

    sorting_params = request.GET.copy()
    if 'sort_direction' in sorting_params:
        del sorting_params['sort_direction']
    if 'sort_by' in sorting_params:
        del sorting_params['sort_by']
    if 'page' in sorting_params:
        del sorting_params['page']

    context.update({
        'paginated_queryset': paginated_queryset,
        'paginated_data_list': serialized_data['data_list'],
        'headers': headers,
        'fields': fields,
        'fields_loading': serializer_class.POST_RELATED_FIELDS,
        'fields_unsortable': serializer_class.UNSORTABLE_FIELDS,
        'fields_hidden': hidden_fields,
        'sorting_params': sorting_params,
    })

    context['rows_number'] = len(
        set(x for x, _ in context.get('fields')) - set(context.get('fields'))
    )

    context['data_count'] = paginated_queryset.paginator.count

    if include_total:
        context.update(
            count_totals(queryset, serializer_class, total_with_fields)
        )
    return context
